[{"content":"In the fast-paced world of DevOps and automation, efficiency and reliability are paramount. Today, we delve into a Docker-based automation script that epitomizes these qualities. Our focus is a python script that utilizes SeleniumBase to navigate through multi-page JavaScript forms to identify available appointments.\nThis script not only operates seamlessly in the background, checking for appointments, but also notifies users via email, attaching screenshots upon successful detection. This process builds on our previous discussions on deploying applications using Docker, illustrating the breadth and versatility of containerization in modern development workflows.\nFind the complete setup on instructions in the GitHub repository.\nYour browser does not support the video tag. Selenium: The Backbone of Web Automation Selenium is more than just a tool; it\u0026rsquo;s a comprehensive framework that enables developers to automate web browser interactions. Imagine you\u0026rsquo;re testing a web application that sells concert tickets. Selenium allows you to write a script that mimics a user purchasing a ticket, from selecting a seat to filling out payment information. This ensures that your application behaves as expected across different web browsers and operating systems.\nThe Role of Selenium in DevOps and CI/CD In a continuous deployment pipeline, automated testing is crucial. Selenium automates the testing of web applications, fitting perfectly into the CI/CD pipeline. For example, each time a developer commits changes to the codebase, Selenium tests can automatically run to verify that the web application still meets all functional requirements. This immediate feedback is vital for maintaining software quality in a fast-paced development environment.\nSelenium vs. SeleniumBase: Enhancing Automation While Selenium sets the stage for web automation, SeleniumBase takes it a step further by simplifying test case creation. With Selenium, you might write extensive code to navigate through a form and verify its contents. SeleniumBase encapsulates common patterns, such as waiting for elements to become visible or clicking elements, into simpler, more robust commands. This not only makes the test code cleaner but also more resilient to changes in the web application.\nUsing SeleniumBase, a script to check for available appointments might navigate through a series of dropdowns to select a location and service type with just a few lines of code, handling waits and clicks more efficiently than raw Selenium code.\nWith plain Selenium, selecting a dropdown would require this code,\n# Tramite select wait.until(EC.visibility_of_element_located((By.ID, \u0026#39;tramiteGrupo[0]\u0026#39;))) # Wait for second AJAX page to Load dropdown2 = wait.until(EC.element_to_be_clickable((By.ID, \u0026#39;tramiteGrupo[0]\u0026#39;))) dropdown2.click() # Ensure the dropdown is opened, might be necessary to trigger JavaScript select_element2 = Select(dropdown2) select_element2.select_by_visible_text(\u0026#39;Toma de huella (expedicion de tarjeta). renovacion de tarjeta larga duracion y duplicado\u0026#39;) option = wait.until(EC.visibility_of_element_located((By.XPATH, \u0026#34;//select[@id=\u0026#39;tramiteGrupo[0]\u0026#39;]/option[contains(text(Toma de huella (expedicion de tarjeta)))]\u0026#34;))) driver.execute_script(\u0026#34;arguments[0].click();\u0026#34;,option) Using SeleniumBase methods, the code above is simply,\nsb.select_option_by_text(\u0026#34;#tramiteGrupo\\\\[0\\\\]\u0026#34;, config[\u0026#39;tramiteOptionText\u0026#39;]) Selenium IDE Using tools such as Selenium IDE we can manually navigate the form completion process and record the CSS selectors, streamlining the conversion to SeleniumBase\u0026rsquo;s API later on.\nNavigating with SeleniumBase SeleniumBase offers the uc mode, which employs undetected-chromedriver to evade detection mechanisms that websites use to block automated browsers. This is particularly useful for appointment checking on sites with anti-bot measures.\nDocker\u0026rsquo;s Role in Streamlining Development Docker encapsulates applications and their environments into containers, ensuring consistency across development, testing, and production. In the context of web scraping and automation, Docker ensures that your SeleniumBase scripts run in an environment where all dependencies are met, regardless of the host system.\nDockerizing the Python Script The Dockerfile starts from a Ubuntu base image, installs Chrome and the necessary drivers, and adds the script. This container can then be run on any system with Docker, without the need for additional setup.\n# SeleniumBase Docker Image FROM ubuntu:18.04 WORKDIR /cita-checker/SeleniumBase #======================================= # Install Python and Basic Python Tools #======================================= RUN apt-get -o Acquire::Check-Valid-Until=false -o Acquire::Check-Date=false update RUN apt-get install -y python3 python3-pip python3-setuptools python3-dev python-distribute RUN alias python=python3 RUN echo \u0026#34;alias python=python3\u0026#34; \u0026gt;\u0026gt; ~/.bashrc #================ # Install Chrome #================ RUN curl -sS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \u0026amp;\u0026amp; \\ echo \u0026#34;deb http://dl.google.com/linux/chrome/deb/ stable main\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/google-chrome.list \u0026amp;\u0026amp; \\ apt-get -yqq update \u0026amp;\u0026amp; \\ apt-get -yqq install google-chrome-stable \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* With a submodule configuration other collaborators can fetch the script with git clone --recurse-submodules https://github.com/tbalza/cita-checker.git and create custom features, such as VNC access for manual interaction, while keeping the original SB repo untouched, streamlining development.\n#=========================================== # Install VNC Server, Window Manager, NoVNC #=========================================== ENV DISPLAY=:99.0 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ python-numpy \\ net-tools \\ x11vnc \\ xfce4 \\ xfce4-goodies \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN mkdir -p /.novnc \u0026amp;\u0026amp; cd /.novnc \\ \u0026amp;\u0026amp; wget -qO- https://github.com/novnc/noVNC/archive/v1.0.0.tar.gz | tar xz --strip 1 -C $PWD \\ \u0026amp;\u0026amp; mkdir /.novnc/utils/websockify \\ \u0026amp;\u0026amp; wget -qO- https://github.com/novnc/websockify/archive/v0.6.1.tar.gz | tar xz --strip 1 -C /.novnc/utils/websockify \\ \u0026amp;\u0026amp; ln -s /.novnc/vnc.html /.novnc/index.html EXPOSE 5900 EXPOSE 6080 The steps for a developer to set up this submodule configuration are:\n# Initialize main project repository cd /cita-checker git init git add . git commit -m \u0026#34;Initial commit\u0026#34; # Add remote repository git remote add origin https://github.com/tbalza/cita-checker git push -u origin master # Add sumbodule git submodule add https: //github.com/seleniumbase/SeleniumBase SeleniumBase git add . git commit -m \u0026#34;Add seleniumbase submodule\u0026#34; git push With an entry point we can define the commands that will keep all our extra services running, tying up all the configuration. In our setup configuring Xvfb is crucial for our desktop environment and VNC to work alongside the SeleniumBase webdriver in headful mode.\n#!/bin/bash # Function to keep the Xvfb running function keepUpScreen() { echo \u0026#34;Running keepUpScreen()\u0026#34; while true; do sleep 1 if [ -z \u0026#34;$(pidof Xvfb)\u0026#34; ]; then echo \u0026#34;Xvfb is not running. Starting Xvfb...\u0026#34; Xvfb :99 -screen 0 1600x900x16 \u0026amp; fi done } # Configure and start Xvfb export DISPLAY=:99.0 rm -f /tmp/.X99-lock \u0026amp;\u0026gt;/dev/null # remove the lock file for X server display number 99 Xvfb :99 -screen 0 1600x900x16 \u0026amp; # Start xfce startxfce4 \u0026amp; # Start x11vnc without a password, accessible only from localhost x11vnc -display :99 -rfbport 5900 -nopw -forever \u0026amp; # Keep Xvfb running keepUpScreen \u0026amp; # Wait for any background processes to finish wait $! # Execute commands passed to the Docker container exec \u0026#34;$@\u0026#34; Advantages of a Containerized Approach By dockerizing your appointment checking script, you not only ensure it runs in a consistent environment but also simplify deployment and scaling. If the script needs to run at multiple locations or at scale, Docker containers can be deployed across multiple machines or cloud instances seamlessly.\nPython Script for Appointment Checking The provided Python script utilizes SeleniumBase\u0026rsquo;s features to navigate a web form and check for appointments. It employs strategies like setting a random window size to mimic human behavior, running in headful mode to cater to the need for manual interaction, and using the uc mode to evade bot detection.\nA Closer Look at the Script\u0026rsquo;s Logic\nSetting Browser Settings and Opening the Appointment Page The script starts by opening the target URL with a random window size to avoid automation detection.\ndef set_random_window_size(sb): min_width = 800 # Define minimum width max_width = 1600 # Define maximum width width = random.randint(min_width, max_width) # Choose a random width within the range height = (width * 2) // 3 # Calculate the height based on a 3:2 aspect ratio sb.set_window_size(width, height) # Set the window size with the calculated width and height def check_for_appointments(): with SB( chromium_arg=\u0026#34;--force-device-scale-factor=1\u0026#34;, # Needed to set window size browser=\u0026#34;chrome\u0026#34;, # When running brave, leave it as chrome. Invokes chromedriver with default options # binary_location=\u0026#34;/usr/bin/brave-browser\u0026#34;, # Uncomment for Brave Browser headed=True, # Run tests in headed/GUI mode on Linux. (To have access to browser after) uc=True, # Use undetected-chromedriver to evade bot-detection. Only works for Chrome/Brave use_auto_ext=False, # Hide chrome\u0026#39;s automation extension slow=True, # Makes actions run slower incognito=True, # Enable Chromium\u0026#39;s Incognito mode. Clear session cookies ) as sb: try: # Clicking logic to get to appointment status set_random_window_size(sb) # Adjust the browser window size (to avoid rate-limiting) Navigation and navigating through the form by selecting options and clicking buttons, demonstrating how SeleniumBase simplifies interaction with web elements.\n) as sb: try: # Clicking logic to get to appointment status set_random_window_size(sb) # Adjust the browser window size (to avoid rate-limiting) sb.open(config[\u0026#39;url\u0026#39;]) # Fetches values.json sb.click(\u0026#34;#form\u0026#34;) sb.select_option_by_text(\u0026#34;#form\u0026#34;, \u0026#34;Barcelona\u0026#34;) sb.click(\u0026#34;#btnAceptar\u0026#34;) sb.select_option_by_text(\u0026#34;#tramiteGrupo\\\\[0\\\\]\u0026#34;, config[\u0026#39;tramiteOptionText\u0026#39;]) # Fetches values.json sb.click(\u0026#34;#btnAceptar\u0026#34;) sb.click(\u0026#34;#btnEntrar\u0026#34;) sb.type(\u0026#34;#txtIdCitado\u0026#34;, config[\u0026#39;idCitadoValue\u0026#39;]) # Fetches values.json sb.type(\u0026#34;#txtDesCitado\u0026#34;, config[\u0026#39;desCitadoValue\u0026#39;]) # Fetches values.json sb.select_option_by_text(\u0026#34;#txtPaisNac\u0026#34;, config[\u0026#39;paisNacValue\u0026#39;]) # Fetches values.json sb.click(\u0026#34;#btnEnviar\u0026#34;) sb.click(\u0026#34;#btnEnviar\u0026#34;) Checking for Availability It then checks for text indicating whether appointments are available. If not found, it assumes availability and proceeds to take a screenshot for evidence, showcasing SeleniumBase\u0026rsquo;s ability to easily interact with web pages and perform checks. # Checking for appointment availability if sb.is_text_visible(\u0026#34;En este momento no hay citas disponibles.\u0026#34;, \u0026#34;div.mf-main--content.ac-custom-content p\u0026#34;): logging.info(\u0026#34;No available appointments. Trying again in 10 minutes.\u0026#34;) return \u0026#34;retry\u0026#34; else: # If the text is not found, assume an appointment is available sb.set_window_size(1280, 1024) # Set correct resolution for screenshot sb.save_screenshot(\u0026#34;/tmp/cita_disponible.png\u0026#34;) # Take a screenshot for the email attachment send_email(\u0026#34;Cita Disponible Alert\u0026#34;, \u0026#34;VNC to vnc://127.0.0.1:5900 to complete\u0026#34;, attach_screenshot=True) logging.info(\u0026#34;Appointments might be available. Keeping the browser open for manual check.\u0026#34;) user_input = input(\u0026#34;Type \u0026#39;restart\u0026#39; and enter\u0026#34;) if user_input.lower() == \u0026#34;restart\u0026#34;: # SB Context manager quits automatically. workaround to maintain open return \u0026#34;manual_check_needed\u0026#34; except Exception as e: logging.error(f\u0026#34;Encountered an error during the steps: {e}. Trying again in 10 minutes.\u0026#34;) return \u0026#34;error\u0026#34; Email Notifications Upon finding an available appointment, the script sends an email notification with the screenshot attached. This demonstrates integrating SeleniumBase with other Python libraries for complete automation solutions.\ndef send_email(subject, message, attach_screenshot=False): # Load email configuration from values.json with open(\u0026#39;/tmp/values.json\u0026#39;, \u0026#39;r\u0026#39;) as file: config = json.load(file) sender_email = config[\u0026#39;sender_email\u0026#39;] receiver_email = config[\u0026#39;receiver_email\u0026#39;] password = config[\u0026#39;password\u0026#39;] smtp_server = config[\u0026#39;smtp_server\u0026#39;] smtp_port = config[\u0026#39;smtp_port\u0026#39;] # Create the email message msg = EmailMessage() msg[\u0026#39;Subject\u0026#39;] = subject msg[\u0026#39;From\u0026#39;] = sender_email # Fetches values.json msg[\u0026#39;To\u0026#39;] = receiver_email # Fetches values.json msg.set_content(message) # Attach the screenshot if required if attach_screenshot: screenshot_path = \u0026#34;/tmp/cita_disponible.png\u0026#34; if os.path.exists(screenshot_path): with open(screenshot_path, \u0026#39;rb\u0026#39;) as f: file_data = f.read() file_name = os.path.basename(screenshot_path) msg.add_attachment(file_data, maintype=\u0026#39;image\u0026#39;, subtype=\u0026#39;png\u0026#39;, filename=file_name) # Send the email try: with smtplib.SMTP_SSL(smtp_server, smtp_port) as smtp: # Fetches values.json smtp.login(sender_email, password) # Fetches values.json smtp.send_message(msg) logging.info(\u0026#34;Email sent successfully!\u0026#34;) except Exception as e: logging.error(f\u0026#34;Error sending email: {e}\u0026#34;) Once the script finds an available appointment you\u0026rsquo;ll receive an email alert with a screenshot: You can set up a distinct alarm tone on your mobile phone for a particular sender to alert you during off-hours, as if you were on call.\nConclusion This detailed exploration showcases the power and versatility of using Docker and SeleniumBase for automating the task of checking for available appointments. Through practical examples and explanations, we\u0026rsquo;ve seen how these technologies can streamline the development and deployment of automation scripts, ensuring reliability, efficiency, and scalability in modern web application testing and automation tasks.\n","permalink":"https://tbalza.net/leveraging-docker-and-selenium-for-efficient-appointment-checking/","summary":"This article explores an automation use-case with Selenium and Docker to periodically access a multi-page js form and check for available appointments.","title":"Leveraging Docker and Selenium for Efficient Appointment Checking"},{"content":"This guide outlines the steps to launch a highly available WordPress application, structured as a four-tier architecture, using ECS for container orchestration.\nOur final infrastructure will look like this,\nFind the complete setup on instructions in the GitHub repository.\nWordPress WordPress is a widely used CMS powering a significant portion of the internet\u0026rsquo;s websites.\nWordPress rose to popularity quickly because of its up-to-date development framework, extensive feature set, flexibility, rapid and multilingual publishing ability, multi-author support, and thriving community. Thousands of free and commercial themes and plugins are available to extend and personalize WordPress for just about every situation. (From bitnami.com)\nTraditionally, WordPress operates as a monolithic application. However, by leveraging Docker, AWS RDS, and EFS, we can transform it into a stateless architecture. This decouples the computing layer from the rest of the infrastructure, enhancing flexibility. Specifically, it allows us to run multiple servers behind load balancers, thereby achieving high availability.\nIn this setup, Docker containers play a crucial role by encapsulating the WordPress application, making it portable and consistent across different computing environments. This ties into the broader architectural tiers by ensuring the application layer is both scalable and resilient.\nFurthermore, the separation of storage (via EFS) and database services (using RDS) enables more robust and isolated backups, as well as seamless rolling updates. This approach ensures that ongoing user sessions remain unaffected, eliminating downtime during updates.\nAdopting Modern Development Practices In our projects, we prioritize technologies that embrace modern development methodologies, including Continuous Integration/Continuous Deployment (CI/CD) and Infrastructure as Code (IaC). We advocate for the following practices:\nAutomated deployment and provisioning of all infrastructure components. Seamless rolling updates for services, ensuring zero downtime. Version-controlled infrastructure changes, treat your infrastructure setup as you would with application code. Unified codebase catering to multiple environments like development and production. Comprehensive monitoring capabilities for crucial components and resources, utilizing metrics for performance insights. Automation Focus\nThis guide primarily covers automation techniques for deployments, project structuring, while deferring in-depth development principles to another discussion.\nPlease note, while AWS resources may lead to costs, our Terraform-guided process simplifies the removal of all resources with the terraform destroy command to prevent unnecessary expenses.\nContainer Orchestration with ECS on EC2\nECS on EC2 provides a versatile environment for running Docker containerized services, supporting applications of any scale or type. Our setup involves managing EC2 instances, ensuring service internet accessibility and high availability by deploying across multiple Availability Zones with dynamic scaling through an Autoscaling Policy.\nECS EC2 versus Fargate\nWhile Fargate offers a serverless alternative, eliminating infrastructure management, it may lead to higher costs with extensive memory usage. ECS on EC2, conversely, grants more flexibility in instance types and running specific tasks like daemon agents. The choice between ECS on EC2 and Fargate hinges on your team\u0026rsquo;s expertise, infrastructure management capabilities, and specific needs such as running specialized software.\nKey Components of Our Setup WordPress: This is the CMS platform we\u0026rsquo;ll be deploying in our environment. Elastic Container Service (ECS): Utilizes clusters, services, and tasks for service operation. Elastic File System (EFS): Ensures persistent application data. Relational Database Service (RDS): Manages database operations. Secrets Manager: Secures sensitive information like passwords. Certificate Manager: Manages SSL certificates, with domain DNS handled externally by CloudFlare. Application Load Balancer (ALB): Directs requests to various container instances. Autoscaling Group (ASG): Provides EC2 instance scaling and high availability. Security Groups: Implements firewall rules for EC2 instances and ALBs. Virtual Private Cloud (VPC): Encapsulates subnets and internet gateways, distinguishing between public subnets for the ALB and private subnets for ECS services. Permissions Requirements In cloud environments like AWS, permissions and roles serve as foundational elements in securing and defining what resources and actions various entities (like services, users, or applications) are allowed or denied. Here\u0026rsquo;s a high-level explanation in relation to the setup:\nSecurity Groups: Act as virtual firewalls that control the traffic allowed to and from ECS containers, ensuring only authorized network requests (like those from an ALB) are permitted. ECS Task Role: Assigns specific permissions to ECS tasks, enabling them to interact with other AWS services like S3 for storage, CloudWatch for logging, or EFS for file systems without embedding sensitive credentials within the containers. Trust Relationship Policy: Establishes a trust where ECS tasks are permitted to assume a role, enabling actions like pulling images or managing resources based on the defined permissions (like AmazonECSTaskExecutionRolePolicy). EC2 Role: Similar to the ECS Task Role, it assigns permissions to the EC2 instances within the cluster to interact with AWS services necessary for their operation, like EFS or the EC2 Container Service itself. Listeners and Target Group: Direct traffic from the ALB to the appropriate backend services based on the request, enabling secure and efficient traffic management. A heads-up: Grasping the following details requires a thorough understanding of each AWS service and the complex ways in which different permission levels interweave. Below is a service-by-service breakdown, detailing the necessary configurations to facilitate communication between individual services.\n1. Amazon Elastic Container Service (ECS):\nECS Security Group (Inbound): Allow traffic from the Application Load Balancer (ALB). ECS Security Group (Outbound): Allow traffic to Amazon RDS, ECS, and the internet. ECS Task Role: Allows ECS tasks to interact with specific AWS services. AmazonECSTaskExecutionRolePolicy: Allows ECS tasks to make API requests to ECS. AmazonS3FullAccess: Required if the setup needs S3 access for backups or media storage. CloudWatchLogsFullAccess: Allows tasks to send logs to Amazon CloudWatch. AmazonElasticFileSystemClientFullAccess: Grants access to EFS. Trust Relationship Policy: Principal: ecs-tasks.amazonaws.com Action: sts:AssumeRole ECS Task Execution Role: Used by: ECS agent and Docker daemon to pull images and manage resources. AmazonECSTaskExecutionRolePolicy Trust Relationship Policy: Principal: ecs-tasks.amazonaws.com Action: sts:AssumeRole 2. Amazon EC2 Instance (from EC2 cluster):\nEC2 Role (defined in ASG module): AmazonEC2ContainerServiceforEC2Role AmazonSSMManagedInstanceCore AmazonElasticFileSystemClientFullAccess EC2 Security Group (defined in ASG module): Inbound: Allow traffic on ports 80, 3306, and 2049 from VPC private subnets. 3. Amazon RDS:\nSecurity Group: Inbound: Allow traffic from the ECS Security Group on MySQL port 3306. 4. Application Load Balancer (ALB):\nSecurity Group: Inbound: Allow traffic on ports 80 (HTTP) and 443 (HTTPS). Outbound: Allow rules to communicate with VPC CIDR block. Listeners: HTTP 80: Redirect to HTTPS 443. HTTPS 443: Forward to the ECS target group. Target Group: Backend protocol: HTTP. Backend port: 8080 5. Auto Scaling Group (ASG):\nSecurity Group: Use the EC2 security group. Policies Required for Associated Roles: AmazonEC2AutoScalingFullAccess 6. Amazon Elastic File System (EFS):\nSecurity Groups: Inbound: Allow traffic on port 2049 from VPC private subnets. Mount Targets: Create in each Availability Zone used by ECS instances or tasks. Overall, permissions and roles are essential in this setup for ensuring that each component operates securely, interacts properly with other services, and has access to only the resources necessary for its function, following the principle of least privilege.\nTerraform Registry Modules This configuration utilizes modules to enhance the efficiency and organization of infrastructure provisioning.\nA module, a superior construct within Terraform, consolidates multiple resources into a single, reusable package. Available from sources such as the Terraform Registry, these modules embody best practices and facilitate consistent application across various projects. They are crafted from resources—the elemental components of Terraform like compute instances or databases—which are the foundational elements for sculpting the anticipated state of infrastructure.\nAdvantages of Utilizing Modules:\nSimplification: Consolidates various resources and methodologies into a unified entity. Standardization: Ensures uniform creation of resources. Versioning: Facilitates seamless updates and straightforward rollbacks. Community-Tested: Guarantees dependability through extensive community review. Maintenance: Streamlines the update and maintenance processes. Documentation: Offers comprehensive instructions and examples. While modules cater to a broad range of needs, direct use of provider resources is warranted for highly customized scenarios or specific requirements not addressed by existing modules. Nonetheless, a hybrid approach, combining modules with direct resource utilization, can often provide the most comprehensive solution, filling in any gaps to achieve the desired infrastructure configuration.\nIn essence, modules render Terraform configurations more streamlined and standardized, striking an optimal balance between ease of use and the ability to tailor to specific needs.\nVPC module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; name = local.name cidr = local.vpc_cidr azs = local.azs public_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)] private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 3)] database_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 6)] create_database_subnet_group = true enable_nat_gateway = true single_nat_gateway = true } output \u0026#34;alb\u0026#34; { value = module.alb.dns_name } This module efficiently establishes a secure, highly available AWS VPC with structured network segmentation (public, private, database subnets) enhancing resilience across multiple zones. Public subnets connect directly to the internet via an Internet Gateway. Private and database subnets, routed for internal traffic, use a NAT Gateway, enabling internet access without external exposure.\nECS Cluster module \u0026#34;ecs_cluster\u0026#34; { source = \u0026#34;terraform-aws-modules/ecs/aws//modules/cluster\u0026#34; version = \u0026#34;~\u0026gt; 5.10\u0026#34; cluster_name = local.name cloudwatch_log_group_retention_in_days = 1 # Capacity provider - autoscaling groups default_capacity_provider_use_fargate = false autoscaling_capacity_providers = { # On-demand instances ex_1 = { auto_scaling_group_arn = module.autoscaling[\u0026#34;ex_1\u0026#34;].autoscaling_group_arn managed_termination_protection = \u0026#34;DISABLED\u0026#34; managed_scaling = { # Enabling would allow for autoscaling instances maximum_scaling_step_size = 1 minimum_scaling_step_size = 1 status = \u0026#34;DISABLED\u0026#34; target_capacity = 100 } default_capacity_provider_strategy = { # This would be edited for multiple capacity providers weight = 60 base = 20 } } } } Since we are not yet using auto-scaling, this module only defines the cluster name and the capacity provider, that will be used later on by the service. (The provider strategy is a placeholder for later implementation)\nECS Service module \u0026#34;ecs_service\u0026#34; { source = \u0026#34;terraform-aws-modules/ecs/aws//modules/service\u0026#34; version = \u0026#34;~\u0026gt; 5.10\u0026#34; # Service name = local.name cluster_arn = module.ecs_cluster.arn cpu = 1843 # CPU units for the task, reserving some for system overhead (t3.micro) memory = 724 # Memory for the task, reserving system overhead (t3.micro) # Task Definition (The Task IAM role is defined in the Autoscaling module) requires_compatibilities = [\u0026#34;EC2\u0026#34;] capacity_provider_strategy = { # On-demand instances ex_1 = { capacity_provider = module.ecs_cluster.autoscaling_capacity_providers[\u0026#34;ex_1\u0026#34;].name weight = 1 base = 1 } } # EFS volume volume = { wordpress_data = { efs_volume_configuration = { file_system_id = module.efs.id transit_encryption = \u0026#34;ENABLED\u0026#34; authorization_config = { access_point_id = module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;] iam = \u0026#34;ENABLED\u0026#34; } } } } load_balancer = { service = { target_group_arn = module.alb.target_groups[\u0026#34;ex_ecs\u0026#34;].arn container_name = local.container_name container_port = local.container_port } } subnet_ids = module.vpc.private_subnets security_group_rules = { alb_http_ingress = { type = \u0026#34;ingress\u0026#34; from_port = local.container_port to_port = local.container_port protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;Service port\u0026#34; source_security_group_id = module.alb.security_group_id }, \u0026#34;allow-internet\u0026#34; = { # to fetch docker image outside AWS (not in ECR) type = \u0026#34;egress\u0026#34; protocol = \u0026#34;all\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] from_port = 0 to_port = 0 } } } Using a t3.micro instance for testing purposes, here we\u0026rsquo;re maxing out a single task (minus the aws agent overhead) to occupy the total available computing resources (CPU and memory). Additionally, we define a snippet of the container definition, where we configure the first stage for using an EFS volume for data storage. Furthermore, we connect the ecs task to the load balancer telling it will receive traffic on port 8080.\nThe volume definition allows the docker container to have WordPress data persistence outside the container itself.\nIf we were using a docker-compose.yml file this chunk,\nvolume = { wordpress_data = { efs_volume_configuration = { file_system_id = module.efs.id transit_encryption = \u0026#34;ENABLED\u0026#34; authorization_config = { access_point_id = module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;] iam = \u0026#34;ENABLED\u0026#34; } } } } Would be the ECS equivalent of this,\nContainer Definitions # Container definition(s) container_definitions = { (local.container_name) = { image = \u0026#34;docker.io/bitnami/wordpress:6\u0026#34; user = \u0026#34;root\u0026#34; # Ideally run docker container as non-root user essential = true port_mappings = [ { name = local.container_name containerPort = local.container_port # Port 8080 is bitnami container default hostPort = local.container_port # ALB listens on port 80, forwards to target group on port 8080 } ] cpu = 1843 # Allocate nearly all CPU units to this container memory = 724 # Allocate majority of memory to this container environment = [ { name = \u0026#34;WORDPRESS_DATABASE_HOST\u0026#34; value = module.rds.db_instance_address }, { name = \u0026#34;WORDPRESS_DATABASE_PORT_NUMBER\u0026#34; value = 3306 }, { name = \u0026#34;WORDPRESS_DATABASE_USER\u0026#34; value = \u0026#34;wordpress\u0026#34; }, { name = \u0026#34;WORDPRESS_DATABASE_NAME\u0026#34; value = \u0026#34;wordpressdb\u0026#34; }, { name = \u0026#34;WORDPRESS_USERNAME\u0026#34; value = \u0026#34;wordpress\u0026#34; }, { name = \u0026#34;WORDPRESS_ENABLE_HTTPS\u0026#34; value = \u0026#34;yes\u0026#34; }, { name = \u0026#34;WORDPRESS_VOLUME_DIR\u0026#34; value = \u0026#34;/mnt/efs/\u0026#34; }, ] secrets = [ { name = \u0026#34;WORDPRESS_DATABASE_PASSWORD\u0026#34; valueFrom = module.secret_db_password.secret_arn }, { name = \u0026#34;WORDPRESS_PASSWORD\u0026#34; valueFrom = module.secret_user_password.secret_arn } ] mount_points = [ { sourceVolume = \u0026#34;wordpress_data\u0026#34;, containerPath = \u0026#34;/bitnami/wordpress\u0026#34; # by default wp-config.php wp-content are persisted, env WORDPRESS_DATA_TO_PERSIST readOnly = false } ] readonly_root_filesystem = false enable_cloudwatch_logging = true create_cloudwatch_log_group = true cloudwatch_log_group_name = \u0026#34;/aws/ecs/${local.name}/${local.container_name}\u0026#34; cloudwatch_log_group_retention_in_days = 1 log_configuration = { logDriver = \u0026#34;awslogs\u0026#34; } } } The current Bitnami image used for deploying an ECS EC2 application offers a ready-to-use, standardized solution that reduces setup time and maintains consistency across different settings.\nConversely, an extensive CI/CD pipeline using Docker provides significantly more customization and control. In this framework, developers can tailor Docker images to specific requirements, update and manage versions through a Version Control System (VCS), and monitor modifications over time.\nFor our purposes, this image supports customization through environment variables specified in the container definitions.\nAdditionally, we set the parameters for cloudwatch to log the events related to our task.\nALB # Extract the last version of AWS Optimized AMI for ECS data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;ecs_optimized_ami\u0026#34; { name = \u0026#34;/aws/service/ecs/optimized-ami/amazon-linux-2/recommended\u0026#34; } module \u0026#34;alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;~\u0026gt; 9.0\u0026#34; name = local.name load_balancer_type = \u0026#34;application\u0026#34; vpc_id = module.vpc.vpc_id subnets = module.vpc.public_subnets # For example only enable_deletion_protection = false # Security Group security_group_ingress_rules = { all_http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;HTTP web traffic\u0026#34; cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; } all_https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;HTTPS web traffic\u0026#34; cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; } } security_group_egress_rules = { all = { ip_protocol = \u0026#34;-1\u0026#34; cidr_ipv4 = module.vpc.vpc_cidr_block } } listeners = { ex-http-https-redirect = { port = 80 protocol = \u0026#34;HTTP\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } ex-https = { port = 443 protocol = \u0026#34;HTTPS\u0026#34; ssl_policy = \u0026#34;ELBSecurityPolicy-TLS13-1-2-Res-2021-06\u0026#34; certificate_arn = module.acm.acm_certificate_arn forward = { target_group_key = \u0026#34;ex_ecs\u0026#34; } } } target_groups = { ex_ecs = { backend_protocol = \u0026#34;HTTP\u0026#34; backend_port = local.container_port target_type = \u0026#34;ip\u0026#34; deregistration_delay = 5 load_balancing_cross_zone_enabled = true health_check = { enabled = true healthy_threshold = 5 interval = 30 matcher = \u0026#34;200\u0026#34; path = \u0026#34;/\u0026#34; port = \u0026#34;traffic-port\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout = 5 unhealthy_threshold = 2 } # Theres nothing to attach here in this definition. Instead, # ECS will attach the IPs of the tasks to this target group create_attachment = false } } tags = local.tags } This block sets up an AWS Application Load Balancer (ALB) redirecting all HTTP traffic to HTTPS, and then all that TLS traffic to our target group that listens on 8080 (forward to ex_ecs). It also sets up health monitoring.\nAutoscaling module \u0026#34;autoscaling\u0026#34; { source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.5\u0026#34; for_each = { # On-demand instances ex_1 = { instance_type = \u0026#34;t3.micro\u0026#34; use_mixed_instances_policy = false } } name = local.name image_id = jsondecode(data.aws_ssm_parameter.ecs_optimized_ami.value)[\u0026#34;image_id\u0026#34;] instance_type = \u0026#34;t3.micro\u0026#34; instance_market_options = { market_type = \u0026#34;spot\u0026#34; # Lower cost for testing purposes } security_groups = [aws_security_group.autoscaling_sg.id] user_data = base64encode(local.on_demand_user_data) ignore_desired_capacity_changes = true create_iam_instance_profile = true iam_role_name = local.name iam_role_description = \u0026#34;ECS role for ${local.name}\u0026#34; iam_role_policies = { AmazonEC2ContainerServiceforEC2Role = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\u0026#34; AmazonSSMManagedInstanceCore = \u0026#34;arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026#34; EFSFullAccess = aws_iam_policy.efs_full_access.arn RDSFullAccess = aws_iam_policy.rds_full_access.arn ecs-secrets-manager-access = aws_iam_policy.secrets_manager_access.arn } vpc_zone_identifier = module.vpc.private_subnets health_check_type = \u0026#34;EC2\u0026#34; min_size = 2 # Make task Highly Available maintaining 2 ec2 instances across each AZ max_size = 4 # Allow for temporary creation of extra instances while reaching desired capacity desired_capacity = 2 # https://github.com/hashicorp/terraform-provider-aws/issues/12582 autoscaling_group_tags = { AmazonECSManaged = true } # Required for managed_termination_protection = \u0026#34;ENABLED\u0026#34; protect_from_scale_in = false } This further links ECS with the load balancer. It defines our Highly Available desired state, where to instances should be always available redundantly in two different AZs.\nIt sets the required policies to grant permissions to the EC2 instance itself for accessing the ECS container management, SSM for instance configuration (and console access), EFS for file storage access, RDS for database access, and access to the Secrets Manager.\nAdditionally, it defines that a payload should be run on the provisioned ec2 instances before the container service runs.\nBash Script on_demand_user_data = \u0026lt;\u0026lt;-EOT #!/bin/bash yum install -y amazon-efs-utils mkdir -p /mnt/efs mount -t efs -o tls,accesspoint=${module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;]} ${module.efs.id}:/ /mnt/efs echo \u0026#39;${module.efs.id}:/ /mnt/efs efs tls,accesspoint=${module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;]} 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; /etc/ecs/ecs.config ECS_CLUSTER=${local.name} ECS_LOGLEVEL=debug ECS_CONTAINER_INSTANCE_TAGS=${jsonencode(local.tags)} ECS_ENABLE_TASK_IAM_ROLE=true EOF EOT Since we\u0026rsquo;re not on fargate, our ec2 instance needs to know that it belongs to de cluster we defined earlier.\nIt also needs to install the efs-utils agent, and mount it to /mnt/efs before the container runs, so it can successfully mount the persistent volume defined earlier in the container definitions.\nWithout this crucial step or instance won\u0026rsquo;t be ready to connect to the container, and the volume will not mount correctly, and instead create internal volumes on the attached SSD very time it\u0026rsquo;s instantiated,\nSecurity Groups # Security Groups for EC2 instances resource \u0026#34;aws_security_group\u0026#34; \u0026#34;autoscaling_sg\u0026#34; { name = \u0026#34;autoscaling_sg\u0026#34; description = \u0026#34;Autoscaling group security group\u0026#34; vpc_id = module.vpc.vpc_id tags = local.tags } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_http_from_alb\u0026#34; { # HTTP type = \u0026#34;ingress\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_mysql_from_alb\u0026#34; { # RDS type = \u0026#34;ingress\u0026#34; from_port = 3306 to_port = 3306 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id # Adjust this if the source is not the ALB } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_nfs_from_alb\u0026#34; { # EFS type = \u0026#34;ingress\u0026#34; from_port = 2049 to_port = 2049 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id # Adjust this if the source is not the ALB } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;egress_all\u0026#34; { type = \u0026#34;egress\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; # Represents all protocols security_group_id = aws_security_group.autoscaling_sg.id cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] # Allows all outbound traffic } # Role Policies to add to EC2 Role resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;efs_full_access\u0026#34; { name = \u0026#34;EFSFullAccess\u0026#34; description = \u0026#34;Provides full access to EFS\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;elasticfilesystem:*\u0026#34; Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }] }) } # elasticfilesystem:Describe* elasticfilesystem:ClientWrite elasticfilesystem:ClientMount # \u0026#34;Resource\u0026#34;: \u0026#34;${module.efs.arn}\u0026#34; resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;rds_full_access\u0026#34; { name = \u0026#34;RDSFullAccess\u0026#34; description = \u0026#34;Provides full access to RDS\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;rds:*\u0026#34; Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }] }) } This is an example of the trade-off between modules and using direct resources. Since the modules don\u0026rsquo;t directly support connecting RDS and EFS, we can create the necessary security groups for added services to interact.\nRDS module \u0026#34;rds\u0026#34; { source = \u0026#34;terraform-aws-modules/rds/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.5.2\u0026#34; identifier = \u0026#34;rds\u0026#34; engine = \u0026#34;mysql\u0026#34; engine_version = \u0026#34;8.0\u0026#34; family = \u0026#34;mysql8.0\u0026#34; # DB parameter group major_engine_version = \u0026#34;8.0\u0026#34; # DB option group instance_class = \u0026#34;db.t4g.micro\u0026#34; storage_type = \u0026#34;gp3\u0026#34; allocated_storage = 20 max_allocated_storage = 100 availability_zone = module.vpc.azs[0] depends_on = [module.secret_db_password] username = \u0026#34;wordpress\u0026#34; password = data.aws_secretsmanager_secret_version.secret_db_password.secret_string # fetch actual password string from secrets manger db_name = \u0026#34;wordpressdb\u0026#34; port = 3306 manage_master_user_password = false iam_database_authentication_enabled = false skip_final_snapshot = true create_db_option_group = false db_subnet_group_name = module.vpc.database_subnet_group vpc_security_group_ids = [module.security_group.security_group_id] } module \u0026#34;security_group\u0026#34; { source = \u0026#34;terraform-aws-modules/security-group/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; name = local.name description = \u0026#34;EC2 docker access to RDS\u0026#34; vpc_id = module.vpc.vpc_id ingress_with_cidr_blocks = [ { # RDS Ingress from_port = 3306 to_port = 3306 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;MySQL access from within VPC\u0026#34; cidr_blocks = module.vpc.vpc_cidr_block }, ] tags = local.tags } Here we simply create the rds instance, and set the password to be retrieved from the secrets manager later on. The depends_on definition makes sure the module has generated the password before it\u0026rsquo;s executed. This usually isn\u0026rsquo;t required because TF can figure out execution order dependencies, but in this case it is warranted.\nEFS module \u0026#34;efs\u0026#34; { source = \u0026#34;terraform-aws-modules/efs/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.6.1\u0026#34; # File system name = local.name creation_token = local.name encrypted = false #kms_key_arn = module.kms.key_arn performance_mode = \u0026#34;generalPurpose\u0026#34; throughput_mode = \u0026#34;bursting\u0026#34; lifecycle_policy = { transition_to_ia = \u0026#34;AFTER_30_DAYS\u0026#34; transition_to_primary_storage_class = \u0026#34;AFTER_1_ACCESS\u0026#34; } # File system policy attach_policy = true bypass_policy_lockout_safety_check = false # for KMS key management deny_nonsecure_transport = false # Allow non encrypted traffic for testing policy_statements = [ { sid = \u0026#34;Example\u0026#34; actions = [\u0026#34;elasticfilesystem:*\u0026#34;] principals = [ { type = \u0026#34;AWS\u0026#34; identifiers = [\u0026#34;*\u0026#34;] } ] } ] # Replace \u0026#34;*\u0026#34; with [module.ecs_service.tasks_iam_role_arn] # Mount targets / security group mount_targets = { for k, v in zipmap(local.azs, module.vpc.private_subnets) : k =\u0026gt; { subnet_id = v } } security_group_description = \u0026#34;Example EFS security group\u0026#34; security_group_vpc_id = module.vpc.vpc_id security_group_rules = { vpc = { # Relying on the defaults provided for EFS/NFS (2049/TCP + ingress) description = \u0026#34;NFS ingress from VPC private subnets\u0026#34; cidr_blocks = module.vpc.private_subnets_cidr_blocks } } # Access point(s) access_points = { posix = { # Used for strict security controls and user isolation name = \u0026#34;posix-example\u0026#34; posix_user = { gid = 1001 uid = 1001 secondary_gids = [1002] } tags = { Additional = \u0026#34;Wordpress EFS\u0026#34; } } root = { # Default root_directory = { path = \u0026#34;/\u0026#34; creation_info = { owner_gid = 0 # Non-root user is 1001. (as defined in dockerfile) root is 0 owner_uid = 0 # Non-root user is 1001. (as defined in dockerfile) root is 0 permissions = \u0026#34;755\u0026#34; } } } } } When our container can mount and read/write with our EFS filesystem, the volumes from its perspective looks like this. 8 exabytes, tells us about the potential scalability of this service (also warns us of reasons it could get out hand and need monitoring)\nACM # Obtain SSL certificate module \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0.1\u0026#34; domain_name = \u0026#34;wp.tbalza.net\u0026#34; validation_method = \u0026#34;DNS\u0026#34; wait_for_validation = false create_route53_records = false tags = { Name = \u0026#34;wp.tbalza.net\u0026#34; } } Configuring TLS is deceptively simple in this setup. We don\u0026rsquo;t need Route53 at all, since our namecheap.com registrar points to Cloudflare NS records, which can be managed via API tokens that later define our CNAME records. We simply generate the certificate with AWS with DNS verification and pass that on to our next module.\nDNS # Validate ACM SSL certificate. Uses the output of acm module to to complete DNS validation in CloudFlare variable \u0026#34;validation_record_keys\u0026#34; { type = list(string) default = [\u0026#34;record1\u0026#34;, \u0026#34;record2\u0026#34;, \u0026#34;record3\u0026#34;, \u0026#34;record4\u0026#34;] # Adjust based on the number of expected ACM validation records } locals { # The output is a map within a single list item, iterate through to define values acm_validation_records = { for idx, val in module.acm.acm_certificate_domain_validation_options : \u0026#34;record${idx + 1}\u0026#34; =\u0026gt; { name = val.resource_record_name # Will appear as unresolved ref before apply value = val.resource_record_value # Will appear as unresolved ref before apply } } } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;acm_validation\u0026#34; { for_each = local.acm_validation_records zone_id = var.cloudflare_zone_id name = each.value.name # Will appear as unresolved ref before apply type = \u0026#34;CNAME\u0026#34; value = each.value.value # Will appear as unresolved ref before apply ttl = 1 proxied = false } # Create wp. subdomain for ALB. Update DNS (managed by CloudFlare) CNAME to point wp.tbalza.net to ALB provider \u0026#34;cloudflare\u0026#34; { api_token = var.cloudflare_api_token # Ensure you have this variable in your variables.tf or passed in via environment variables } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;wp_cname\u0026#34; { zone_id = var.cloudflare_zone_id # Ensure you have this variable in your variables.tf or passed in via environment variables name = \u0026#34;wp\u0026#34; value = module.alb.dns_name type = \u0026#34;CNAME\u0026#34; proxied = false } Our nanecheap.com domain tbalza.net (which is hosted by GitHub pages, as per the first blog post) was changed to Cloudflare DNS management, by pointed to their NS records.\nWith our Cloudflare API token, we can take the output of module.acm.acm_certificate_domain_validation_options that generates a map within a list and with TF convert this output, to call for the change of the CNAME wp. to be that which is dynamically generated by AWS.\nSecrets Manager data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} # Allow ECS tasks to access secrets resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;secrets_manager_access\u0026#34; { name = \u0026#34;ecs-secrets-manager-access\u0026#34; description = \u0026#34;Allows ECS tasks to access secrets in Secrets Manager\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [ { Action = [ \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34; ], Effect = \u0026#34;Allow\u0026#34;, Resource = \u0026#34;*\u0026#34; # It\u0026#39;s better to specify exact ARNs of secrets } ] }) } # Generate wordpress db password data \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;secret_db_password\u0026#34; { # required to return the actual pw string for the RDS module secret_id = module.secret_db_password.secret_id depends_on = [module.secret_db_password] } module \u0026#34;secret_db_password\u0026#34; { source = \u0026#34;terraform-aws-modules/secrets-manager/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.1.2\u0026#34; name_prefix = \u0026#34;wordpress-db-password-\u0026#34; description = \u0026#34;Secret for WordPress Database Password\u0026#34; create_random_password = true random_password_length = 41 # RDS length limit random_password_override_special = \u0026#34;_\u0026#34; # list of permitted special characters } # Generate wordpress user password data \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;secret_user_password\u0026#34; { secret_id = module.secret_user_password.secret_id depends_on = [module.secret_user_password] } module \u0026#34;secret_user_password\u0026#34; { source = \u0026#34;terraform-aws-modules/secrets-manager/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.1.2\u0026#34; name_prefix = \u0026#34;wordpress-user-password-\u0026#34; description = \u0026#34;Secret for WordPress User Password\u0026#34; create_random_password = true random_password_length = 41 # RDS length limit random_password_override_special = \u0026#34;_\u0026#34; # list of permitted special characters } Conclusions We\u0026rsquo;ve successfully set up a high-availability environment ensuring our application layer is both scalable and resilient. Additionally, by separating storage with EFS and database operations via RDS, we\u0026rsquo;ve ensured stronger, more isolated backups and enabled smoother rolling updates. This design keeps the user experience uninterrupted, significantly reducing downtime during updates.\nAreas for Improvement Autoscaling: Refine autoscaling by adjusting the desired_count based on real-time needs. This ensures our setup can dynamically allocate resources and ECS Tasks according to our Target Tracking Policy without resetting our capacity planning with each deployment.\nPrinciple of Least Privilege: Tighten security by refining ARN/actions/resources to uphold the principle of least privilege, ensuring containers operate securely without root access.\nCloudFront: Optimize our setup further, reducing load on our compute resources by serving static files from closer to the user by adding a caching layer with CloudFront.\nCI/CD Integration: Integrate with CI/CD systems like Jenkins or CircleCI, using unique Git commit hashes to manage deployments. This ensures continuous integration and deployment for every change, maintaining a smooth and consistent update process.\nTagging Strategy: Ensure all resources are properly tagged to streamline billing and management.\nBackup Procedures: Automate backup processes for RDS and EFS will be a priority to safeguard our data effectively.\nMonitoring and Alerts: Implement monitoring solutions with tools like Grafana, coupled with SNS notifications, to stay ahead of scaling events and other significant system events.\nLooking Ahead In our next piece, we\u0026rsquo;ll dive into the implementation of CI/CD systems and explore real-world production scenarios, including Git operations and rolling updates, to further enhance our setup.\n","permalink":"https://tbalza.net/container-orchestration-of-ha-application-with-ecs/","summary":"This guide outlines the steps to launch a highly available WordPress application, structured as a four-tier architecture, using ECS for container orchestration.","title":"Container Orchestration of HA Application with ECS"},{"content":"This guide offers a comprehensive step-by-step process on how to build and deploy an Amazon API Gateway resource using Terraform. It also demonstrates how to invoke a Lambda function through an HTTPS endpoint, which will directly interact with DynamoDB.\nOne of the primary benefits of using Terraform is the capability to launch the entire project with a single click. The principle of infrastructure as code is crucial for projects as it facilitates tracking of configuration changes and encourages efficient collaboration within large teams.\nGetting Started For Mac users, you can install Homebrew and set up AWS CLI, Terraform, and Bruno.\nbrew install awscli brew install terraform brew install bruno # An Opensource API client, we\u0026#39;ll use for testing aws configure # Follow the steps to provide your AWS account credentials (We recommend using a testing account)\ngit clone https://github.com/tbalza/lambda-dynamodb-api.git cd lambda-dynamodb-api terraform init This will initialize terraform, and download all the necessary libraries and modules, enabling our HCL code in main.tf to interact with AWS using the credentials we set up earlier with the CLI. All the necessary files will be stored in the .terraform folder for future use. terraform plan -out tfplan This command outlines all the changes that will occur, assessing the current state of resources and what is specified as a final result in our code. terraform apply -auto-approve tfplan After running the apply command, it will proceed to create and configure our infrastructure as described in our code. Once you\u0026rsquo;re finished, you can tidy up by\nterraform destroy Ideally, you should use terraform plan -destroy -out tfplan and then terraform apply tfplan as this allows you to double-check all changes before committing them to AWS.\nOverview of all the sections Using the commands provided above, you can quickly set up and test the infrastructure. Moreover, you can effortlessly clean it up to prevent any unexpected charges in the future. In the following sections, we\u0026rsquo;ll provide a brief explanation of the function of each block.\nSetup IAM Permissions Create custom IAM policy\nmodule \u0026#34;iam_policy\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-policy\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; name = \u0026#34;lambda-apigateway-role-policy\u0026#34; description = \u0026#34;Custom policy with permission to DynamoDB and CloudWatch Logs\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Sid = \u0026#34;Stmt1428341300017\u0026#34; Action = [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }, { Sid = \u0026#34;\u0026#34; Resource = \u0026#34;*\u0026#34; Action = [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; } ] }) } This is a standalone custom policy that allows the principal who assumes it to edit DynamoDB and CW Logs.\nCreate role for lambda function, attach custom IAM policy and trusted entity\nmodule \u0026#34;iam_assumable_role_lambda\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-assumable-role\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; create_role = true role_name = \u0026#34;lambda-apigateway-role\u0026#34; create_custom_role_trust_policy = true custom_role_trust_policy = data.aws_iam_policy_document.custom_trust_policy.json custom_role_policy_arns = [module.iam_policy.arn] # Get the ARN from the iam_policy module } # Create trusted entity data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;custom_trust_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } } } This creates a role and attaches the previous policy to that role. It also assigns the role the trusted entity required for the lambda role to assume it.\nCreate trigger equivalent to explicitly grant permissions to the API Gateway to invoke your Lambda function\nresource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;apigw\u0026#34; { statement_id = \u0026#34;AllowExecutionFromAPIGateway\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.example.arn principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_api_gateway_deployment.dev.execution_arn}/*\u0026#34; } When creating resources via ClickOps, the trigger is automatically assigned to the lambda function when it is associated with the API, however via CLI we need to explicitly set this in order to have the correct permissions.\nCreate Lambda Function lambda_function.py\nfrom __future__ import print_function import boto3 import json print(\u0026#39;Loading function\u0026#39;) def lambda_handler(event, context): \u0026#39;\u0026#39;\u0026#39;Provide an event that contains the following keys: - operation: one of the operations in the operations dict below - tableName: required for operations that interact with DynamoDB - payload: a parameter to pass to the operation being performed \u0026#39;\u0026#39;\u0026#39; #print(\u0026#34;Received event: \u0026#34; + json.dumps(event, indent=2)) operation = event[\u0026#39;operation\u0026#39;] if \u0026#39;tableName\u0026#39; in event: dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;).Table(event[\u0026#39;tableName\u0026#39;]) operations = { \u0026#39;create\u0026#39;: lambda x: dynamo.put_item(**x), \u0026#39;read\u0026#39;: lambda x: dynamo.get_item(**x), \u0026#39;update\u0026#39;: lambda x: dynamo.update_item(**x), \u0026#39;delete\u0026#39;: lambda x: dynamo.delete_item(**x), \u0026#39;list\u0026#39;: lambda x: dynamo.scan(**x), \u0026#39;echo\u0026#39;: lambda x: x, \u0026#39;ping\u0026#39;: lambda x: \u0026#39;pong\u0026#39; } if operation in operations: return operations[operation](event.get(\u0026#39;payload\u0026#39;)) else: raise ValueError(\u0026#39;Unrecognized operation \u0026#34;{}\u0026#34;\u0026#39;.format(operation)) This lambda function is written in Python, and works with any table name we specify via the POST method. Actions such as, create, read, update, delete, list, echo and ping, are supported.\nZip the lambda_function.py to enable uploading to AWS\ndata \u0026#34;archive_file\u0026#34; \u0026#34;lambda_zip\u0026#34; { type = \u0026#34;zip\u0026#34; source_file = \u0026#34;${path.module}/lambda_function.py\u0026#34; output_path = \u0026#34;${path.module}/lambda_function.zip\u0026#34; } We use built in TF tools to zip the Python script, which is needed in order to upload it to AWS via this method.\nCreate lambda function from lambda_function.py\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;example\u0026#34; { function_name = \u0026#34;LambdaFunctionsOverHttps\u0026#34; handler = \u0026#34;lambda_function.lambda_handler\u0026#34; runtime = \u0026#34;python3.12\u0026#34; filename = data.archive_file.lambda_zip.output_path # Associate function to previously created role role = module.iam_assumable_role_lambda.iam_role_arn # Get the ARN from the iam_assumable_role_lambda module } Finally, we upload the lambda function and attach the role we created earlier.\nDynamoDB Create a simple dynamodb table\nmodule \u0026#34;dynamodb_table\u0026#34; { source = \u0026#34;terraform-aws-modules/dynamodb-table/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0.1\u0026#34; name = \u0026#34;lambda-apigateway\u0026#34; hash_key = \u0026#34;id\u0026#34; # primary key attributes = [ { name = \u0026#34;id\u0026#34; type = \u0026#34;S\u0026#34; } ] } This creates a table with an \u0026ldquo;id\u0026rdquo; primary column that will expect strings.\nAPI Create API\nresource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;DynamoDBOperations\u0026#34; { name = \u0026#34;DynamoDBOperations\u0026#34; description = \u0026#34;API for DynamoDB Operations\u0026#34; api_key_source = \u0026#34;HEADER\u0026#34; endpoint_configuration { types = [\u0026#34;REGIONAL\u0026#34;] } } This creates the API by itself, and defines the end point configuration type.\nCreate resource\nresource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;DynamoDBManager\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id parent_id = aws_api_gateway_rest_api.DynamoDBOperations.root_resource_id path_part = \u0026#34;dynamodbmanager\u0026#34; } These are the various parts of your API that clients can access. They are represented as a hierarchical structure similar to a file path. For example, in the API endpoint https://api.example.com/users/profile, \u0026lsquo;users\u0026rsquo; and \u0026lsquo;profile\u0026rsquo; are resources. Resources can have child resources, creating a tree-like structure.\nCreate POST method\nresource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;post\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = \u0026#34;POST\u0026#34; authorization = \u0026#34;NONE\u0026#34; api_key_required = false } These are the HTTP methods (also known as verbs) that clients can use to interact with the resources. Common methods include GET, POST, PUT, DELETE, and PATCH. Each method represents a different type of operation that can be performed on a resource. For example, a GET method on a \u0026lsquo;users\u0026rsquo; resource might retrieve a list of users, while a POST method on the same resource might create a new user.\nLink API to Lambda function\nresource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;lambda\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method integration_http_method = \u0026#34;POST\u0026#34; type = \u0026#34;AWS\u0026#34; uri = aws_lambda_function.example.invoke_arn passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; } uri = aws_lambda_function.example.invoke_arn: This line sets the URI of the integrated backend. In this case, it\u0026rsquo;s a Lambda function, and the ARN (Amazon Resource Name) used to invoke the function is retrieved from another resource named example of type aws_lambda_function.\nCreate response code\nresource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;response\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = \u0026#34;200\u0026#34; } http_method = aws_api_gateway_method.post.http_method: This line sets the HTTP method for the method response. The method is retrieved from another resource of type aws_api_gateway_method with the name post.\nstatus_code = \u0026ldquo;200\u0026rdquo;: This line sets the HTTP status code for the method response. In this case, it\u0026rsquo;s \u0026ldquo;200\u0026rdquo;, which typically represents a successful HTTP request.\nIntegrate response code\nresource \u0026#34;aws_api_gateway_integration_response\u0026#34; \u0026#34;lambda\u0026#34; { depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = aws_api_gateway_method_response.response.status_code } depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response]: This line specifies that the creation of this resource depends on the successful creation of other resources in order the execute correctly.\nThis creates an integration response that depends on the successful creation of the API Gateway integration and method response.\nBoth blocks are part of configuring an API Gateway to handle responses from a backend service, in this case, a Lambda function.\nDeploy in \u0026ldquo;Dev\u0026rdquo;\nresource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;dev\u0026#34; { depends_on = [aws_api_gateway_integration.lambda] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id stage_name = \u0026#34;dev\u0026#34; } This deploys our API to the \u0026ldquo;dev\u0026rdquo; stage, generating our invoke URL that we will use to interact with the API\nOutput API Invoke URL output \u0026#34;api_invoke_url\u0026#34; { description = \u0026#34;api_invoke_url\u0026#34; value = \u0026#34;${aws_api_gateway_deployment.dev.invoke_url}/${aws_api_gateway_resource.DynamoDBManager.path_part}\u0026#34; } This line from outputs.tf prints out our Invoke URL generated earlier, and displays it after terraform apply.\nTest with API Client Open Bruno. Create Collection \u0026gt; New Request \u0026gt; Paste the invoke URL generated inside URL: POST\nThen in the BODY section, select Raw \u0026gt; JSON, past the code below, and press cmd+enter to execute\n{ \u0026#34;operation\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;lambda-apigateway\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1234ABCD\u0026#34;, \u0026#34;number\u0026#34;: 88 } } } Verify Results in DynamoDB Go to DynamoDB \u0026gt; Tables \u0026gt; lambda-apigateway \u0026gt; Explore Table Items Here you\u0026rsquo;ll see that our POST payload, sent to the API Invoke URL, interacts with Lambda, which in turn modifies our DynamoDB table.\nClean up Run this command to delete all our previously created resources and configurations.\nKey Takeaways We\u0026rsquo;ve created an API that is publicly accessible via our Invoke URL, which allows us to interact with the DynamoDB table directly via a Lambda function.\nIn future articles, we\u0026rsquo;ll delve deeper into GitOps, and CI/CD pipelines building on this knowledge, and implement authentication, DNS and certificate configurations.\n","permalink":"https://tbalza.net/using-terraform-for-amazon-api-gateway-deployment/","summary":"This article offers a comprehensive tutorial on creating and deploying an Amazon API Gateway resource using Terraform, and demonstrates how to invoke a Lambda function through an HTTPS endpoint.","title":"Using Terraform for Amazon API Gateway Deployment"},{"content":"In this guide, we\u0026rsquo;ll walk you through the process of creating a static website that won\u0026rsquo;t burden you with storage costs or egress fees\nWe\u0026rsquo;ll also help you set up a custom domain that will automatically reflect any changes made whenever a new commit is detected in the main branch of our version control system.\nThe Problem with Egress Traffic Whether you\u0026rsquo;re operating an EC2 instance or directly serving files via S3, egress traffic can come with a price tag. For instance, AWS charges around $0.023 per GB. While this might not seem like much for personal pages, without the necessary precautions like DDoS protection or rate limiting, you could potentially face a hefty bill running into the thousands of dollars in the event of an attack. Fortunately, this won\u0026rsquo;t be an issue with our setup.\nThe Solution: Free Tier Services This is why it\u0026rsquo;s a smart move to utilize services like GitHub Pages and CloudFlare for a public personal site. These platforms don\u0026rsquo;t charge for outgoing traffic, even on their free tier service, making them an ideal solution.\nWhy Choose Hugo? Hugo is a static site generator written in Go. It\u0026rsquo;s not only incredibly fast but also simple to set up. The resulting pages load swiftly and are SEO optimized right from the start. This makes Hugo a fantastic alternative to platforms like WordPress, where publishing static content can be a bit of a hassle in terms of maintenance, costs, and performance.\nConfigure Your Personal Domain First, purchase your domain with services such as namecheap.com, then you can access the advanced settings and configure the records that will point to the GitHub Page that will reference to later Set up the A Records and CNAME just like in the image above. Reference the Github Pages Apex Configuration for further details. You can skip this step if you\u0026rsquo;d wish to deploy the site without a custom domain Set up Hugo Install Hugo and Create a New Site brew install hugo hugo new site tbalza-blog-hugo -f yml Install Hugo with Homebrew\nRun hugo new site \u0026lt;site_name\u0026gt;. This will create a directory \u0026lt;site_name\u0026gt; containing the hugo templates. Pass in the -f yml argument so configuration files are stored in the yml format\nInstall Theme git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) git submodule update --remote --merge Run git init on the root of the project to initialize a Git repository Install the PaperMod theme using Git Submodules theme: [\u0026#34;PaperMod\u0026#34;] Add the theme parameter in hugo.yaml baseURL: https://tbalza.net/ Add the baseURL parameter in hugo.yaml hugo server Run this command from within the project folder. This will create a blank page with no content Configure GitHub Actions to Publish to the GitHub Pages Push the First Commit, Create gh-pages Branch git commit -m \u0026#34;first commit\u0026#34; git branch -M main # rename master to main git remote add origin https://github.com/tbalza/tbalza-blog-hugo # replace with your repo git push -u origin main git branch gh-pages # used internally by github to execute the deployment action, will throw error if not created Create a repository on GitHub, add the remote address, and push your first commit, create gh-pages branch Allow Read and Write Permissions Under Settings \u0026gt; Actions \u0026gt; General \u0026gt; Workflow permissions, enable \u0026ldquo;Read and write permissions\u0026rdquo; Set up Custom Domain in GitHub Pages Configure your custom domain under Settings \u0026gt; Pages \u0026gt; Custom Domain Create deploy.yml mkdir -p .github/workflows cd .github/workflows touch deploy.yml In you project root folder create .github/workflows/deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.123.4/hugo_0.123.4_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;tbalza\u0026#39; git config user.email \u0026#39;tomas.balza@gmail.com\u0026#39; git commit -m \u0026#39;Updated site\u0026#39; git push Checkout source This action checks-out your repository under $GITHUB_WORKSPACE, so your workflow can access it. submodules:true ensures that our submodule for the theme repository is fetched as well Checkout destination The second step allows us to reference the gh-pages branch via the $GITHUB_WORKSPACE/built-site directory, where our static sites will be stored in Setup Hugo This step downloads and extracts the Hugo static site generator. It uses curl to download a specific version of Hugo from its GitHub releases page, and tar to extract the downloaded file. Build This step runs Hugo to build your static site. The built site\u0026rsquo;s files are placed in the public directory. Deploy This step deploys the built site to the gh-pages branch. It first copies the built site\u0026rsquo;s files from the public directory to the built-site directory. It then changes the current directory to built-site, stages all changes for commit with git add ., sets the Git username and email, commits the changes with a message of \u0026lsquo;Updated site\u0026rsquo;, and finally pushes the commit to the gh-pages branch. The gh-pages branch is special because GitHub Pages serves the contents of this branch at your GitHub Pages URL. The if: github.ref == 'refs/heads/main' condition ensures that the site is only deployed when changes are pushed to the main branch.\nAfter the deploy.yml workflow is pushed, any future changes in the main branch, will trigger Hugo to generate an updated /public/ folder and serve it in GitHub Pages.\n","permalink":"https://tbalza.net/blog-deployment-on-github-pages-using-github-actions/","summary":"In this article we will go through the steps of setting up a static website, that will not incur in storage costs or egress fees.","title":"Blog Deployment on Github Pages using Github Actions"}]