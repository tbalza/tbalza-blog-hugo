[{"content":"This guide outlines the steps to launch a highly available WordPress application, structured as a four-tier architecture, using ECS for container orchestration.\nOur final infrastructure will look like this,\nFind the complete setup on instructions in the GitHub repository.\nWordPress WordPress is a widely used CMS powering a significant portion of the internet\u0026rsquo;s websites.\nWordPress rose to popularity quickly because of its up-to-date development framework, extensive feature set, flexibility, rapid and multilingual publishing ability, multi-author support, and thriving community. Thousands of free and commercial themes and plugins are available to extend and personalize WordPress for just about every situation. (From bitnami.com)\nTraditionally, WordPress operates as a monolithic application. However, by leveraging Docker, AWS RDS, and EFS, we can transform it into a stateless architecture. This decouples the computing layer from the rest of the infrastructure, enhancing flexibility. Specifically, it allows us to run multiple servers behind load balancers, thereby achieving high availability.\nIn this setup, Docker containers play a crucial role by encapsulating the WordPress application, making it portable and consistent across different computing environments. This ties into the broader architectural tiers by ensuring the application layer is both scalable and resilient.\nFurthermore, the separation of storage (via EFS) and database services (using RDS) enables more robust and isolated backups, as well as seamless rolling updates. This approach ensures that ongoing user sessions remain unaffected, eliminating downtime during updates.\nAdopting Modern Development Practices In our projects, we prioritize technologies that embrace modern development methodologies, including Continuous Integration/Continuous Deployment (CI/CD) and Infrastructure as Code (IaC). We advocate for the following practices:\nAutomated deployment and provisioning of all infrastructure components. Seamless rolling updates for services, ensuring zero downtime. Version-controlled infrastructure changes, treat your infrastructure setup as you would with application code. Unified codebase catering to multiple environments like development and production. Comprehensive monitoring capabilities for crucial components and resources, utilizing metrics for performance insights. Automation Focus\nThis guide primarily covers automation techniques for deployments, project structuring, while deferring in-depth development principles to another discussion.\nPlease note, while AWS resources may lead to costs, our Terraform-guided process simplifies the removal of all resources with the terraform destroy command to prevent unnecessary expenses.\nContainer Orchestration with ECS on EC2\nECS on EC2 provides a versatile environment for running Docker containerized services, supporting applications of any scale or type. Our setup involves managing EC2 instances, ensuring service internet accessibility and high availability by deploying across multiple Availability Zones with dynamic scaling through an Autoscaling Policy.\nECS EC2 versus Fargate\nWhile Fargate offers a serverless alternative, eliminating infrastructure management, it may lead to higher costs with extensive memory usage. ECS on EC2, conversely, grants more flexibility in instance types and running specific tasks like daemon agents. The choice between ECS on EC2 and Fargate hinges on your team\u0026rsquo;s expertise, infrastructure management capabilities, and specific needs such as running specialized software.\nKey Components of Our Setup WordPress: This is the CMS platform we\u0026rsquo;ll be deploying in our environment. Elastic Container Service (ECS): Utilizes clusters, services, and tasks for service operation. Elastic File System (EFS): Ensures persistent application data. Relational Database Service (RDS): Manages database operations. Secrets Manager: Secures sensitive information like passwords. Certificate Manager: Manages SSL certificates, with domain DNS handled externally by CloudFlare. Application Load Balancer (ALB): Directs requests to various container instances. Autoscaling Group (ASG): Provides EC2 instance scaling and high availability. Security Groups: Implements firewall rules for EC2 instances and ALBs. Virtual Private Cloud (VPC): Encapsulates subnets and internet gateways, distinguishing between public subnets for the ALB and private subnets for ECS services. Permissions Requirements In cloud environments like AWS, permissions and roles serve as foundational elements in securing and defining what resources and actions various entities (like services, users, or applications) are allowed or denied. Here\u0026rsquo;s a high-level explanation in relation to the setup:\nSecurity Groups: Act as virtual firewalls that control the traffic allowed to and from ECS containers, ensuring only authorized network requests (like those from an ALB) are permitted. ECS Task Role: Assigns specific permissions to ECS tasks, enabling them to interact with other AWS services like S3 for storage, CloudWatch for logging, or EFS for file systems without embedding sensitive credentials within the containers. Trust Relationship Policy: Establishes a trust where ECS tasks are permitted to assume a role, enabling actions like pulling images or managing resources based on the defined permissions (like AmazonECSTaskExecutionRolePolicy). EC2 Role: Similar to the ECS Task Role, it assigns permissions to the EC2 instances within the cluster to interact with AWS services necessary for their operation, like EFS or the EC2 Container Service itself. Listeners and Target Group: Direct traffic from the ALB to the appropriate backend services based on the request, enabling secure and efficient traffic management. 1. Amazon Elastic Container Service (ECS):\nECS Security Group (Inbound): Allow traffic from the Application Load Balancer (ALB). ECS Security Group (Outbound): Allow traffic to Amazon RDS, ECS, and the internet. ECS Task Role: Allows ECS tasks to interact with specific AWS services. AmazonECSTaskExecutionRolePolicy: Allows ECS tasks to make API requests to ECS. AmazonS3FullAccess: Required if the setup needs S3 access for backups or media storage. CloudWatchLogsFullAccess: Allows tasks to send logs to Amazon CloudWatch. AmazonElasticFileSystemClientFullAccess: Grants access to EFS. Trust Relationship Policy: Principal: ecs-tasks.amazonaws.com Action: sts:AssumeRole ECS Task Execution Role: Used by: ECS agent and Docker daemon to pull images and manage resources. AmazonECSTaskExecutionRolePolicy Trust Relationship Policy: Principal: ecs-tasks.amazonaws.com Action: sts:AssumeRole 2. Amazon EC2 Instance (from EC2 cluster):\nEC2 Role (defined in ASG module): AmazonEC2ContainerServiceforEC2Role AmazonSSMManagedInstanceCore AmazonElasticFileSystemClientFullAccess EC2 Security Group (defined in ASG module): Inbound: Allow traffic on ports 80, 3306, and 2049 from VPC private subnets. 3. Amazon RDS:\nSecurity Group: Inbound: Allow traffic from the ECS Security Group on MySQL/PostgreSQL port (typically 3306/5432). 4. Application Load Balancer (ALB):\nSecurity Group: Inbound: Allow traffic on ports 80 (HTTP) and 443 (HTTPS). Outbound: Allow rules to communicate with VPC CIDR block. Listeners: HTTP 80: Redirect to HTTPS 443. HTTPS 443: Forward to the ECS target group. Target Group: Backend protocol: HTTP. Backend port: 8080 5. Auto Scaling Group (ASG):\nSecurity Group: Use the EC2 security group. Policies Required for Associated Roles: AmazonEC2AutoScalingFullAccess 6. Amazon Elastic File System (EFS):\nSecurity Groups: Inbound: Allow traffic on port 2049 from VPC private subnets. Mount Targets: Create in each Availability Zone used by ECS instances or tasks. Overall, permissions and roles are essential in this setup for ensuring that each component operates securely, interacts properly with other services, and has access to only the resources necessary for its function, following the principle of least privilege.\nTerraform Registry Modules This configuration utilizes modules to enhance the efficiency and organization of infrastructure provisioning.\nA module, a superior construct within Terraform, consolidates multiple resources into a single, reusable package. Available from sources such as the Terraform Registry, these modules embody best practices and facilitate consistent application across various projects. They are crafted from resources—the elemental components of Terraform like compute instances or databases—which are the foundational elements for sculpting the anticipated state of infrastructure.\nAdvantages of Utilizing Modules:\nSimplification: Consolidates various resources and methodologies into a unified entity. Standardization: Ensures uniform creation of resources. Versioning: Facilitates seamless updates and straightforward rollbacks. Community-Tested: Guarantees dependability through extensive community review. Maintenance: Streamlines the update and maintenance processes. Documentation: Offers comprehensive instructions and examples. While modules cater to a broad range of needs, direct use of provider resources is warranted for highly customized scenarios or specific requirements not addressed by existing modules. Nonetheless, a hybrid approach, combining modules with direct resource utilization, can often provide the most comprehensive solution, filling in any gaps to achieve the desired infrastructure configuration.\nIn essence, modules render Terraform configurations more streamlined and standardized, striking an optimal balance between ease of use and the ability to tailor to specific needs.\nVPC module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; name = local.name cidr = local.vpc_cidr azs = local.azs public_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)] private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 3)] database_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 6)] create_database_subnet_group = true enable_nat_gateway = true single_nat_gateway = true } output \u0026#34;alb\u0026#34; { value = module.alb.dns_name } ECS Cluster module \u0026#34;ecs_cluster\u0026#34; { source = \u0026#34;terraform-aws-modules/ecs/aws//modules/cluster\u0026#34; version = \u0026#34;~\u0026gt; 5.10\u0026#34; cluster_name = local.name cloudwatch_log_group_retention_in_days = 1 # Capacity provider - autoscaling groups default_capacity_provider_use_fargate = false autoscaling_capacity_providers = { # On-demand instances ex_1 = { auto_scaling_group_arn = module.autoscaling[\u0026#34;ex_1\u0026#34;].autoscaling_group_arn managed_termination_protection = \u0026#34;DISABLED\u0026#34; managed_scaling = { # Enabling would allow for autoscaling instances maximum_scaling_step_size = 1 minimum_scaling_step_size = 1 status = \u0026#34;DISABLED\u0026#34; target_capacity = 100 } default_capacity_provider_strategy = { # This would be edited for multiple capacity providers weight = 60 base = 20 } } } tags = local.tags } ECS Service module \u0026#34;ecs_service\u0026#34; { source = \u0026#34;terraform-aws-modules/ecs/aws//modules/service\u0026#34; version = \u0026#34;~\u0026gt; 5.10\u0026#34; # Service name = local.name cluster_arn = module.ecs_cluster.arn cpu = 1843 # CPU units for the task, reserving some for system overhead (t3.micro) memory = 724 # Memory for the task, reserving system overhead (t3.micro) # Task Definition (The Task IAM role is defined in the Autoscaling module) requires_compatibilities = [\u0026#34;EC2\u0026#34;] capacity_provider_strategy = { # On-demand instances ex_1 = { capacity_provider = module.ecs_cluster.autoscaling_capacity_providers[\u0026#34;ex_1\u0026#34;].name weight = 1 base = 1 } } # EFS volume volume = { wordpress_data = { efs_volume_configuration = { file_system_id = module.efs.id transit_encryption = \u0026#34;ENABLED\u0026#34; authorization_config = { access_point_id = module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;] iam = \u0026#34;ENABLED\u0026#34; } } } } load_balancer = { service = { target_group_arn = module.alb.target_groups[\u0026#34;ex_ecs\u0026#34;].arn container_name = local.container_name container_port = local.container_port } } subnet_ids = module.vpc.private_subnets security_group_rules = { alb_http_ingress = { type = \u0026#34;ingress\u0026#34; from_port = local.container_port to_port = local.container_port protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;Service port\u0026#34; source_security_group_id = module.alb.security_group_id }, \u0026#34;allow-internet\u0026#34; = { # to fetch docker image outside AWS (not in ECR) type = \u0026#34;egress\u0026#34; protocol = \u0026#34;all\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] from_port = 0 to_port = 0 } } } Container Definitions # Container definition(s) container_definitions = { (local.container_name) = { image = \u0026#34;docker.io/bitnami/wordpress:6\u0026#34; user = \u0026#34;root\u0026#34; # Ideally run docker container as non-root user essential = true port_mappings = [ { name = local.container_name containerPort = local.container_port # Port 8080 is bitnami container default hostPort = local.container_port # ALB listens on port 80, forwards to target group on port 8080 } ] cpu = 1843 # Allocate nearly all CPU units to this container memory = 724 # Allocate majority of memory to this container environment = [ { name = \u0026#34;WORDPRESS_DATABASE_HOST\u0026#34; value = module.rds.db_instance_address }, { name = \u0026#34;WORDPRESS_DATABASE_PORT_NUMBER\u0026#34; value = 3306 }, { name = \u0026#34;WORDPRESS_DATABASE_USER\u0026#34; value = \u0026#34;wordpress\u0026#34; }, { name = \u0026#34;WORDPRESS_DATABASE_NAME\u0026#34; value = \u0026#34;wordpressdb\u0026#34; }, { name = \u0026#34;WORDPRESS_USERNAME\u0026#34; value = \u0026#34;wordpress\u0026#34; }, { name = \u0026#34;WORDPRESS_ENABLE_HTTPS\u0026#34; value = \u0026#34;yes\u0026#34; }, { name = \u0026#34;WORDPRESS_VOLUME_DIR\u0026#34; value = \u0026#34;/mnt/efs/\u0026#34; }, ] secrets = [ { name = \u0026#34;WORDPRESS_DATABASE_PASSWORD\u0026#34; valueFrom = module.secret_db_password.secret_arn }, { name = \u0026#34;WORDPRESS_PASSWORD\u0026#34; valueFrom = module.secret_user_password.secret_arn } ] mount_points = [ { sourceVolume = \u0026#34;wordpress_data\u0026#34;, containerPath = \u0026#34;/bitnami/wordpress\u0026#34; # by default wp-config.php wp-content are persisted, env WORDPRESS_DATA_TO_PERSIST readOnly = false } ] readonly_root_filesystem = false enable_cloudwatch_logging = true create_cloudwatch_log_group = true cloudwatch_log_group_name = \u0026#34;/aws/ecs/${local.name}/${local.container_name}\u0026#34; cloudwatch_log_group_retention_in_days = 1 log_configuration = { logDriver = \u0026#34;awslogs\u0026#34; } } } ALB # Extract the last version of AWS Optimized AMI for ECS data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;ecs_optimized_ami\u0026#34; { name = \u0026#34;/aws/service/ecs/optimized-ami/amazon-linux-2/recommended\u0026#34; } module \u0026#34;alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;~\u0026gt; 9.0\u0026#34; name = local.name load_balancer_type = \u0026#34;application\u0026#34; vpc_id = module.vpc.vpc_id subnets = module.vpc.public_subnets # For example only enable_deletion_protection = false # Security Group security_group_ingress_rules = { all_http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;HTTP web traffic\u0026#34; cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; } all_https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;HTTPS web traffic\u0026#34; cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; } } security_group_egress_rules = { all = { ip_protocol = \u0026#34;-1\u0026#34; cidr_ipv4 = module.vpc.vpc_cidr_block } } listeners = { ex-http-https-redirect = { port = 80 protocol = \u0026#34;HTTP\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } ex-https = { port = 443 protocol = \u0026#34;HTTPS\u0026#34; ssl_policy = \u0026#34;ELBSecurityPolicy-TLS13-1-2-Res-2021-06\u0026#34; certificate_arn = module.acm.acm_certificate_arn forward = { target_group_key = \u0026#34;ex_ecs\u0026#34; } } } target_groups = { ex_ecs = { backend_protocol = \u0026#34;HTTP\u0026#34; backend_port = local.container_port target_type = \u0026#34;ip\u0026#34; deregistration_delay = 5 load_balancing_cross_zone_enabled = true health_check = { enabled = true healthy_threshold = 5 interval = 30 matcher = \u0026#34;200\u0026#34; path = \u0026#34;/\u0026#34; port = \u0026#34;traffic-port\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout = 5 unhealthy_threshold = 2 } # Theres nothing to attach here in this definition. Instead, # ECS will attach the IPs of the tasks to this target group create_attachment = false } } tags = local.tags } Autoscaling module \u0026#34;autoscaling\u0026#34; { source = \u0026#34;terraform-aws-modules/autoscaling/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.5\u0026#34; for_each = { # On-demand instances ex_1 = { instance_type = \u0026#34;t3.micro\u0026#34; use_mixed_instances_policy = false } } name = local.name image_id = jsondecode(data.aws_ssm_parameter.ecs_optimized_ami.value)[\u0026#34;image_id\u0026#34;] instance_type = \u0026#34;t3.micro\u0026#34; instance_market_options = { market_type = \u0026#34;spot\u0026#34; # Lower cost for testing purposes } security_groups = [aws_security_group.autoscaling_sg.id] user_data = base64encode(local.on_demand_user_data) ignore_desired_capacity_changes = true create_iam_instance_profile = true iam_role_name = local.name iam_role_description = \u0026#34;ECS role for ${local.name}\u0026#34; iam_role_policies = { AmazonEC2ContainerServiceforEC2Role = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\u0026#34; AmazonSSMManagedInstanceCore = \u0026#34;arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026#34; EFSFullAccess = aws_iam_policy.efs_full_access.arn RDSFullAccess = aws_iam_policy.rds_full_access.arn ecs-secrets-manager-access = aws_iam_policy.secrets_manager_access.arn } vpc_zone_identifier = module.vpc.private_subnets health_check_type = \u0026#34;EC2\u0026#34; min_size = 2 # Make task Highly Available maintaining 2 ec2 instances across each AZ max_size = 4 # Allow for temporary creation of extra instances while reaching desired capacity desired_capacity = 2 # https://github.com/hashicorp/terraform-provider-aws/issues/12582 autoscaling_group_tags = { AmazonECSManaged = true } # Required for managed_termination_protection = \u0026#34;ENABLED\u0026#34; protect_from_scale_in = false } Bash Script on_demand_user_data = \u0026lt;\u0026lt;-EOT #!/bin/bash yum install -y amazon-efs-utils mkdir -p /mnt/efs mount -t efs -o tls,accesspoint=${module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;]} ${module.efs.id}:/ /mnt/efs echo \u0026#39;${module.efs.id}:/ /mnt/efs efs tls,accesspoint=${module.efs.access_points[\u0026#34;root\u0026#34;][\u0026#34;id\u0026#34;]} 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; /etc/ecs/ecs.config ECS_CLUSTER=${local.name} ECS_LOGLEVEL=debug ECS_CONTAINER_INSTANCE_TAGS=${jsonencode(local.tags)} ECS_ENABLE_TASK_IAM_ROLE=true EOF EOT Security Groups # Security Groups for EC2 instances resource \u0026#34;aws_security_group\u0026#34; \u0026#34;autoscaling_sg\u0026#34; { name = \u0026#34;autoscaling_sg\u0026#34; description = \u0026#34;Autoscaling group security group\u0026#34; vpc_id = module.vpc.vpc_id tags = local.tags } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_http_from_alb\u0026#34; { # HTTP type = \u0026#34;ingress\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_mysql_from_alb\u0026#34; { # RDS type = \u0026#34;ingress\u0026#34; from_port = 3306 to_port = 3306 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id # Adjust this if the source is not the ALB } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;ingress_nfs_from_alb\u0026#34; { # EFS type = \u0026#34;ingress\u0026#34; from_port = 2049 to_port = 2049 protocol = \u0026#34;tcp\u0026#34; security_group_id = aws_security_group.autoscaling_sg.id source_security_group_id = module.alb.security_group_id # Adjust this if the source is not the ALB } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;egress_all\u0026#34; { type = \u0026#34;egress\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; # Represents all protocols security_group_id = aws_security_group.autoscaling_sg.id cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] # Allows all outbound traffic } # Role Policies to add to EC2 Role resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;efs_full_access\u0026#34; { name = \u0026#34;EFSFullAccess\u0026#34; description = \u0026#34;Provides full access to EFS\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;elasticfilesystem:*\u0026#34; Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }] }) } # elasticfilesystem:Describe* elasticfilesystem:ClientWrite elasticfilesystem:ClientMount # \u0026#34;Resource\u0026#34;: \u0026#34;${module.efs.arn}\u0026#34; resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;rds_full_access\u0026#34; { name = \u0026#34;RDSFullAccess\u0026#34; description = \u0026#34;Provides full access to RDS\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;rds:*\u0026#34; Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }] }) } RDS module \u0026#34;rds\u0026#34; { source = \u0026#34;terraform-aws-modules/rds/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.5.2\u0026#34; identifier = \u0026#34;rds\u0026#34; engine = \u0026#34;mysql\u0026#34; engine_version = \u0026#34;8.0\u0026#34; family = \u0026#34;mysql8.0\u0026#34; # DB parameter group major_engine_version = \u0026#34;8.0\u0026#34; # DB option group instance_class = \u0026#34;db.t4g.micro\u0026#34; storage_type = \u0026#34;gp3\u0026#34; allocated_storage = 20 max_allocated_storage = 100 availability_zone = module.vpc.azs[0] depends_on = [module.secret_db_password] username = \u0026#34;wordpress\u0026#34; password = data.aws_secretsmanager_secret_version.secret_db_password.secret_string # fetch actual password string from secrets manger db_name = \u0026#34;wordpressdb\u0026#34; port = 3306 manage_master_user_password = false iam_database_authentication_enabled = false skip_final_snapshot = true create_db_option_group = false db_subnet_group_name = module.vpc.database_subnet_group vpc_security_group_ids = [module.security_group.security_group_id] } module \u0026#34;security_group\u0026#34; { source = \u0026#34;terraform-aws-modules/security-group/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; name = local.name description = \u0026#34;EC2 docker access to RDS\u0026#34; vpc_id = module.vpc.vpc_id ingress_with_cidr_blocks = [ { # RDS Ingress from_port = 3306 to_port = 3306 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;MySQL access from within VPC\u0026#34; cidr_blocks = module.vpc.vpc_cidr_block }, ] tags = local.tags } EFS module \u0026#34;efs\u0026#34; { source = \u0026#34;terraform-aws-modules/efs/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.6.1\u0026#34; # File system name = local.name creation_token = local.name encrypted = false #kms_key_arn = module.kms.key_arn performance_mode = \u0026#34;generalPurpose\u0026#34; throughput_mode = \u0026#34;bursting\u0026#34; lifecycle_policy = { transition_to_ia = \u0026#34;AFTER_30_DAYS\u0026#34; transition_to_primary_storage_class = \u0026#34;AFTER_1_ACCESS\u0026#34; } # File system policy attach_policy = true bypass_policy_lockout_safety_check = false # for KMS key management deny_nonsecure_transport = false # Allow non encrypted traffic for testing policy_statements = [ { sid = \u0026#34;Example\u0026#34; actions = [\u0026#34;elasticfilesystem:*\u0026#34;] principals = [ { type = \u0026#34;AWS\u0026#34; identifiers = [\u0026#34;*\u0026#34;] } ] } ] # Replace \u0026#34;*\u0026#34; with [module.ecs_service.tasks_iam_role_arn] # Mount targets / security group mount_targets = { for k, v in zipmap(local.azs, module.vpc.private_subnets) : k =\u0026gt; { subnet_id = v } } security_group_description = \u0026#34;Example EFS security group\u0026#34; security_group_vpc_id = module.vpc.vpc_id security_group_rules = { vpc = { # Relying on the defaults provided for EFS/NFS (2049/TCP + ingress) description = \u0026#34;NFS ingress from VPC private subnets\u0026#34; cidr_blocks = module.vpc.private_subnets_cidr_blocks } } # Access point(s) access_points = { posix = { # Used for strict security controls and user isolation name = \u0026#34;posix-example\u0026#34; posix_user = { gid = 1001 uid = 1001 secondary_gids = [1002] } tags = { Additional = \u0026#34;Wordpress EFS\u0026#34; } } root = { # Default root_directory = { path = \u0026#34;/\u0026#34; creation_info = { owner_gid = 0 # Non-root user is 1001. (as defined in dockerfile) root is 0 owner_uid = 0 # Non-root user is 1001. (as defined in dockerfile) root is 0 permissions = \u0026#34;755\u0026#34; } } } } } ACM # Obtain SSL certificate module \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0.1\u0026#34; domain_name = \u0026#34;wp.tbalza.net\u0026#34; validation_method = \u0026#34;DNS\u0026#34; wait_for_validation = false create_route53_records = false tags = { Name = \u0026#34;wp.tbalza.net\u0026#34; } } DNS # Validate ACM SSL certificate. Uses the output of acm module to to complete DNS validation in CloudFlare variable \u0026#34;validation_record_keys\u0026#34; { type = list(string) default = [\u0026#34;record1\u0026#34;, \u0026#34;record2\u0026#34;, \u0026#34;record3\u0026#34;, \u0026#34;record4\u0026#34;] # Adjust based on the number of expected ACM validation records } locals { # The output is a map within a single list item, iterate through to define values acm_validation_records = { for idx, val in module.acm.acm_certificate_domain_validation_options : \u0026#34;record${idx + 1}\u0026#34; =\u0026gt; { name = val.resource_record_name # Will appear as unresolved ref before apply value = val.resource_record_value # Will appear as unresolved ref before apply } } } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;acm_validation\u0026#34; { for_each = local.acm_validation_records zone_id = var.cloudflare_zone_id name = each.value.name # Will appear as unresolved ref before apply type = \u0026#34;CNAME\u0026#34; value = each.value.value # Will appear as unresolved ref before apply ttl = 1 proxied = false } # Create wp. subdomain for ALB. Update DNS (managed by CloudFlare) CNAME to point wp.tbalza.net to ALB provider \u0026#34;cloudflare\u0026#34; { api_token = var.cloudflare_api_token # Ensure you have this variable in your variables.tf or passed in via environment variables } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;wp_cname\u0026#34; { zone_id = var.cloudflare_zone_id # Ensure you have this variable in your variables.tf or passed in via environment variables name = \u0026#34;wp\u0026#34; value = module.alb.dns_name type = \u0026#34;CNAME\u0026#34; proxied = false } Secrets Manager data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} # Allow ECS tasks to access secrets resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;secrets_manager_access\u0026#34; { name = \u0026#34;ecs-secrets-manager-access\u0026#34; description = \u0026#34;Allows ECS tasks to access secrets in Secrets Manager\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [ { Action = [ \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34; ], Effect = \u0026#34;Allow\u0026#34;, Resource = \u0026#34;*\u0026#34; # It\u0026#39;s better to specify exact ARNs of secrets } ] }) } # Generate wordpress db password data \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;secret_db_password\u0026#34; { # required to return the actual pw string for the RDS module secret_id = module.secret_db_password.secret_id depends_on = [module.secret_db_password] } module \u0026#34;secret_db_password\u0026#34; { source = \u0026#34;terraform-aws-modules/secrets-manager/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.1.2\u0026#34; name_prefix = \u0026#34;wordpress-db-password-\u0026#34; description = \u0026#34;Secret for WordPress Database Password\u0026#34; create_random_password = true random_password_length = 41 # RDS length limit random_password_override_special = \u0026#34;_\u0026#34; # list of permitted special characters } # Generate wordpress user password data \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;secret_user_password\u0026#34; { secret_id = module.secret_user_password.secret_id depends_on = [module.secret_user_password] } module \u0026#34;secret_user_password\u0026#34; { source = \u0026#34;terraform-aws-modules/secrets-manager/aws\u0026#34; version = \u0026#34;~\u0026gt; 1.1.2\u0026#34; name_prefix = \u0026#34;wordpress-user-password-\u0026#34; description = \u0026#34;Secret for WordPress User Password\u0026#34; create_random_password = true random_password_length = 41 # RDS length limit random_password_override_special = \u0026#34;_\u0026#34; # list of permitted special characters } Conclusions We\u0026rsquo;ve successfully set up a high-availability environment ensuring our application layer is both scalable and resilient. Additionally, by separating storage with EFS and database operations via RDS, we\u0026rsquo;ve ensured stronger, more isolated backups and enabled smoother rolling updates. This design keeps the user experience uninterrupted, significantly reducing downtime during updates.\nAreas for Improvement Autoscaling: Refine autoscaling by adjusting the desired_count based on real-time needs. This ensures our setup can dynamically allocate resources and ECS Tasks according to our Target Tracking Policy without resetting our capacity planning with each deployment.\nPrinciple of Least Privilege: Tighten security by refining ARN/actions/resources to uphold the principle of least privilege, ensuring containers operate securely without root access.\nCloudFront: Optimize our setup further, reducing load on our compute resources by serving static files from closer to the user by adding a caching layer with CloudFront.\nCI/CD Integration: Integrate with CI/CD systems like Jenkins or CircleCI, using unique Git commit hashes to manage deployments. This ensures continuous integration and deployment for every change, maintaining a smooth and consistent update process.\nTagging Strategy: Ensure all resources are properly tagged to streamline billing and management.\nBackup Procedures: Automate backup processes for RDS and EFS will be a priority to safeguard our data effectively.\nMonitoring and Alerts: Implement monitoring solutions with tools like Grafana, coupled with SNS notifications, to stay ahead of scaling events and other significant system events.\nLooking Ahead In our next piece, we\u0026rsquo;ll dive into the implementation of CI/CD systems and explore real-world production scenarios, including Git operations and rolling updates, to further enhance our setup.\n","permalink":"https://tbalza.net/container-orchestration-of-ha-application-with-ecs/","summary":"This guide outlines the steps to launch a highly available WordPress application, structured as a four-tier architecture, using ECS for container orchestration.","title":"Container Orchestration of HA Application with ECS"},{"content":"This guide offers a comprehensive step-by-step process on how to build and deploy an Amazon API Gateway resource using Terraform. It also demonstrates how to invoke a Lambda function through an HTTPS endpoint, which will directly interact with DynamoDB.\nOne of the primary benefits of using Terraform is the capability to launch the entire project with a single click. The principle of infrastructure as code is crucial for projects as it facilitates tracking of configuration changes and encourages efficient collaboration within large teams.\nGetting Started For Mac users, you can install Homebrew and set up AWS CLI, Terraform, and Bruno.\nbrew install awscli brew install terraform brew install bruno # An Opensource API client, we\u0026#39;ll use for testing aws configure # Follow the steps to provide your AWS account credentials (We recommend using a testing account)\ngit clone https://github.com/tbalza/lambda-dynamodb-api.git cd lambda-dynamodb-api terraform init This will initialize terraform, and download all the necessary libraries and modules, enabling our HCL code in main.tf to interact with AWS using the credentials we set up earlier with the CLI. All the necessary files will be stored in the .terraform folder for future use. terraform plan -out tfplan This command outlines all the changes that will occur, assessing the current state of resources and what is specified as a final result in our code. terraform apply -auto-approve tfplan After running the apply command, it will proceed to create and configure our infrastructure as described in our code. Once you\u0026rsquo;re finished, you can tidy up by\nterraform destroy Ideally, you should use terraform plan -destroy -out tfplan and then terraform apply tfplan as this allows you to double-check all changes before committing them to AWS.\nOverview of all the sections Using the commands provided above, you can quickly set up and test the infrastructure. Moreover, you can effortlessly clean it up to prevent any unexpected charges in the future. In the following sections, we\u0026rsquo;ll provide a brief explanation of the function of each block.\nSetup IAM Permissions Create custom IAM policy\nmodule \u0026#34;iam_policy\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-policy\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; name = \u0026#34;lambda-apigateway-role-policy\u0026#34; description = \u0026#34;Custom policy with permission to DynamoDB and CloudWatch Logs\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Sid = \u0026#34;Stmt1428341300017\u0026#34; Action = [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }, { Sid = \u0026#34;\u0026#34; Resource = \u0026#34;*\u0026#34; Action = [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; } ] }) } This is a standalone custom policy that allows the principal who assumes it to edit DynamoDB and CW Logs.\nCreate role for lambda function, attach custom IAM policy and trusted entity\nmodule \u0026#34;iam_assumable_role_lambda\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-assumable-role\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; create_role = true role_name = \u0026#34;lambda-apigateway-role\u0026#34; create_custom_role_trust_policy = true custom_role_trust_policy = data.aws_iam_policy_document.custom_trust_policy.json custom_role_policy_arns = [module.iam_policy.arn] # Get the ARN from the iam_policy module } # Create trusted entity data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;custom_trust_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } } } This creates a role and attaches the previous policy to that role. It also assigns the role the trusted entity required for the lambda role to assume it.\nCreate trigger equivalent to explicitly grant permissions to the API Gateway to invoke your Lambda function\nresource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;apigw\u0026#34; { statement_id = \u0026#34;AllowExecutionFromAPIGateway\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.example.arn principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_api_gateway_deployment.dev.execution_arn}/*\u0026#34; } When creating resources via ClickOps, the trigger is automatically assigned to the lambda function when it is associated with the API, however via CLI we need to explicitly set this in order to have the correct permissions.\nCreate Lambda Function lambda_function.py\nfrom __future__ import print_function import boto3 import json print(\u0026#39;Loading function\u0026#39;) def lambda_handler(event, context): \u0026#39;\u0026#39;\u0026#39;Provide an event that contains the following keys: - operation: one of the operations in the operations dict below - tableName: required for operations that interact with DynamoDB - payload: a parameter to pass to the operation being performed \u0026#39;\u0026#39;\u0026#39; #print(\u0026#34;Received event: \u0026#34; + json.dumps(event, indent=2)) operation = event[\u0026#39;operation\u0026#39;] if \u0026#39;tableName\u0026#39; in event: dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;).Table(event[\u0026#39;tableName\u0026#39;]) operations = { \u0026#39;create\u0026#39;: lambda x: dynamo.put_item(**x), \u0026#39;read\u0026#39;: lambda x: dynamo.get_item(**x), \u0026#39;update\u0026#39;: lambda x: dynamo.update_item(**x), \u0026#39;delete\u0026#39;: lambda x: dynamo.delete_item(**x), \u0026#39;list\u0026#39;: lambda x: dynamo.scan(**x), \u0026#39;echo\u0026#39;: lambda x: x, \u0026#39;ping\u0026#39;: lambda x: \u0026#39;pong\u0026#39; } if operation in operations: return operations[operation](event.get(\u0026#39;payload\u0026#39;)) else: raise ValueError(\u0026#39;Unrecognized operation \u0026#34;{}\u0026#34;\u0026#39;.format(operation)) This lambda function is written in Python, and works with any table name we specify via the POST method. Actions such as, create, read, update, delete, list, echo and ping, are supported.\nZip the lambda_function.py to enable uploading to AWS\ndata \u0026#34;archive_file\u0026#34; \u0026#34;lambda_zip\u0026#34; { type = \u0026#34;zip\u0026#34; source_file = \u0026#34;${path.module}/lambda_function.py\u0026#34; output_path = \u0026#34;${path.module}/lambda_function.zip\u0026#34; } We use built in TF tools to zip the Python script, which is needed in order to upload it to AWS via this method.\nCreate lambda function from lambda_function.py\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;example\u0026#34; { function_name = \u0026#34;LambdaFunctionsOverHttps\u0026#34; handler = \u0026#34;lambda_function.lambda_handler\u0026#34; runtime = \u0026#34;python3.12\u0026#34; filename = data.archive_file.lambda_zip.output_path # Associate function to previously created role role = module.iam_assumable_role_lambda.iam_role_arn # Get the ARN from the iam_assumable_role_lambda module } Finally, we upload the lambda function and attach the role we created earlier.\nDynamoDB Create a simple dynamodb table\nmodule \u0026#34;dynamodb_table\u0026#34; { source = \u0026#34;terraform-aws-modules/dynamodb-table/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0.1\u0026#34; name = \u0026#34;lambda-apigateway\u0026#34; hash_key = \u0026#34;id\u0026#34; # primary key attributes = [ { name = \u0026#34;id\u0026#34; type = \u0026#34;S\u0026#34; } ] } This creates a table with an \u0026ldquo;id\u0026rdquo; primary column that will expect strings.\nAPI Create API\nresource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;DynamoDBOperations\u0026#34; { name = \u0026#34;DynamoDBOperations\u0026#34; description = \u0026#34;API for DynamoDB Operations\u0026#34; api_key_source = \u0026#34;HEADER\u0026#34; endpoint_configuration { types = [\u0026#34;REGIONAL\u0026#34;] } } This creates the API by itself, and defines the end point configuration type.\nCreate resource\nresource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;DynamoDBManager\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id parent_id = aws_api_gateway_rest_api.DynamoDBOperations.root_resource_id path_part = \u0026#34;dynamodbmanager\u0026#34; } These are the various parts of your API that clients can access. They are represented as a hierarchical structure similar to a file path. For example, in the API endpoint https://api.example.com/users/profile, \u0026lsquo;users\u0026rsquo; and \u0026lsquo;profile\u0026rsquo; are resources. Resources can have child resources, creating a tree-like structure.\nCreate POST method\nresource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;post\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = \u0026#34;POST\u0026#34; authorization = \u0026#34;NONE\u0026#34; api_key_required = false } These are the HTTP methods (also known as verbs) that clients can use to interact with the resources. Common methods include GET, POST, PUT, DELETE, and PATCH. Each method represents a different type of operation that can be performed on a resource. For example, a GET method on a \u0026lsquo;users\u0026rsquo; resource might retrieve a list of users, while a POST method on the same resource might create a new user.\nLink API to Lambda function\nresource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;lambda\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method integration_http_method = \u0026#34;POST\u0026#34; type = \u0026#34;AWS\u0026#34; uri = aws_lambda_function.example.invoke_arn passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; } uri = aws_lambda_function.example.invoke_arn: This line sets the URI of the integrated backend. In this case, it\u0026rsquo;s a Lambda function, and the ARN (Amazon Resource Name) used to invoke the function is retrieved from another resource named example of type aws_lambda_function.\nCreate response code\nresource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;response\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = \u0026#34;200\u0026#34; } http_method = aws_api_gateway_method.post.http_method: This line sets the HTTP method for the method response. The method is retrieved from another resource of type aws_api_gateway_method with the name post.\nstatus_code = \u0026ldquo;200\u0026rdquo;: This line sets the HTTP status code for the method response. In this case, it\u0026rsquo;s \u0026ldquo;200\u0026rdquo;, which typically represents a successful HTTP request.\nIntegrate response code\nresource \u0026#34;aws_api_gateway_integration_response\u0026#34; \u0026#34;lambda\u0026#34; { depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = aws_api_gateway_method_response.response.status_code } depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response]: This line specifies that the creation of this resource depends on the successful creation of other resources in order the execute correctly.\nThis creates an integration response that depends on the successful creation of the API Gateway integration and method response.\nBoth blocks are part of configuring an API Gateway to handle responses from a backend service, in this case, a Lambda function.\nDeploy in \u0026ldquo;Dev\u0026rdquo;\nresource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;dev\u0026#34; { depends_on = [aws_api_gateway_integration.lambda] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id stage_name = \u0026#34;dev\u0026#34; } This deploys our API to the \u0026ldquo;dev\u0026rdquo; stage, generating our invoke URL that we will use to interact with the API\nOutput API Invoke URL output \u0026#34;api_invoke_url\u0026#34; { description = \u0026#34;api_invoke_url\u0026#34; value = \u0026#34;${aws_api_gateway_deployment.dev.invoke_url}/${aws_api_gateway_resource.DynamoDBManager.path_part}\u0026#34; } This line from outputs.tf prints out our Invoke URL generated earlier, and displays it after terraform apply.\nTest with API Client Open Bruno. Create Collection \u0026gt; New Request \u0026gt; Paste the invoke URL generated inside URL: POST\nThen in the BODY section, select Raw \u0026gt; JSON, past the code below, and press cmd+enter to execute\n{ \u0026#34;operation\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;lambda-apigateway\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1234ABCD\u0026#34;, \u0026#34;number\u0026#34;: 88 } } } Verify Results in DynamoDB Go to DynamoDB \u0026gt; Tables \u0026gt; lambda-apigateway \u0026gt; Explore Table Items Here you\u0026rsquo;ll see that our POST payload, sent to the API Invoke URL, interacts with Lambda, which in turn modifies our DynamoDB table.\nClean up Run this command to delete all our previously created resources and configurations.\nKey Takeaways We\u0026rsquo;ve created an API that is publicly accessible via our Invoke URL, which allows us to interact with the DynamoDB table directly via a Lambda function.\nIn future articles, we\u0026rsquo;ll delve deeper into GitOps, and CI/CD pipelines building on this knowledge, and implement authentication, DNS and certificate configurations.\n","permalink":"https://tbalza.net/using-terraform-for-amazon-api-gateway-deployment/","summary":"This article offers a comprehensive tutorial on creating and deploying an Amazon API Gateway resource using Terraform, and demonstrates how to invoke a Lambda function through an HTTPS endpoint.","title":"Using Terraform for Amazon API Gateway Deployment"},{"content":"In this guide, we\u0026rsquo;ll walk you through the process of creating a static website that won\u0026rsquo;t burden you with storage costs or egress fees\nWe\u0026rsquo;ll also help you set up a custom domain that will automatically reflect any changes made whenever a new commit is detected in the main branch of our version control system.\nThe Problem with Egress Traffic Whether you\u0026rsquo;re operating an EC2 instance or directly serving files via S3, egress traffic can come with a price tag. For instance, AWS charges around $0.023 per GB. While this might not seem like much for personal pages, without the necessary precautions like DDoS protection or rate limiting, you could potentially face a hefty bill running into the thousands of dollars in the event of an attack. Fortunately, this won\u0026rsquo;t be an issue with our setup.\nThe Solution: Free Tier Services This is why it\u0026rsquo;s a smart move to utilize services like GitHub Pages and CloudFlare for a public personal site. These platforms don\u0026rsquo;t charge for outgoing traffic, even on their free tier service, making them an ideal solution.\nWhy Choose Hugo? Hugo is a static site generator written in Go. It\u0026rsquo;s not only incredibly fast but also simple to set up. The resulting pages load swiftly and are SEO optimized right from the start. This makes Hugo a fantastic alternative to platforms like WordPress, where publishing static content can be a bit of a hassle in terms of maintenance, costs, and performance.\nConfigure Your Personal Domain First, purchase your domain with services such as namecheap.com, then you can access the advanced settings and configure the records that will point to the GitHub Page that will reference to later Set up the A Records and CNAME just like in the image above. Reference the Github Pages Apex Configuration for further details. You can skip this step if you\u0026rsquo;d wish to deploy the site without a custom domain Set up Hugo Install Hugo and Create a New Site brew install hugo hugo new site tbalza-blog-hugo -f yml Install Hugo with Homebrew\nRun hugo new site \u0026lt;site_name\u0026gt;. This will create a directory \u0026lt;site_name\u0026gt; containing the hugo templates. Pass in the -f yml argument so configuration files are stored in the yml format\nInstall Theme git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) git submodule update --remote --merge Run git init on the root of the project to initialize a Git repository Install the PaperMod theme using Git Submodules theme: [\u0026#34;PaperMod\u0026#34;] Add the theme parameter in hugo.yaml baseURL: https://tbalza.net/ Add the baseURL parameter in hugo.yaml hugo server Run this command from within the project folder. This will create a blank page with no content Configure GitHub Actions to Publish to the GitHub Pages Push the First Commit, Create gh-pages Branch git commit -m \u0026#34;first commit\u0026#34; git branch -M main # rename master to main git remote add origin https://github.com/tbalza/tbalza-blog-hugo # replace with your repo git push -u origin main git branch gh-pages # used internally by github to execute the deployment action, will throw error if not created Create a repository on GitHub, add the remote address, and push your first commit, create gh-pages branch Allow Read and Write Permissions Under Settings \u0026gt; Actions \u0026gt; General \u0026gt; Workflow permissions, enable \u0026ldquo;Read and write permissions\u0026rdquo; Set up Custom Domain in GitHub Pages Configure your custom domain under Settings \u0026gt; Pages \u0026gt; Custom Domain Create deploy.yml mkdir -p .github/workflows cd .github/workflows touch deploy.yml In you project root folder create .github/workflows/deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.123.4/hugo_0.123.4_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;tbalza\u0026#39; git config user.email \u0026#39;tomas.balza@gmail.com\u0026#39; git commit -m \u0026#39;Updated site\u0026#39; git push Checkout source This action checks-out your repository under $GITHUB_WORKSPACE, so your workflow can access it. submodules:true ensures that our submodule for the theme repository is fetched as well Checkout destination The second step allows us to reference the gh-pages branch via the $GITHUB_WORKSPACE/built-site directory, where our static sites will be stored in Setup Hugo This step downloads and extracts the Hugo static site generator. It uses curl to download a specific version of Hugo from its GitHub releases page, and tar to extract the downloaded file. Build This step runs Hugo to build your static site. The built site\u0026rsquo;s files are placed in the public directory. Deploy This step deploys the built site to the gh-pages branch. It first copies the built site\u0026rsquo;s files from the public directory to the built-site directory. It then changes the current directory to built-site, stages all changes for commit with git add ., sets the Git username and email, commits the changes with a message of \u0026lsquo;Updated site\u0026rsquo;, and finally pushes the commit to the gh-pages branch. The gh-pages branch is special because GitHub Pages serves the contents of this branch at your GitHub Pages URL. The if: github.ref == 'refs/heads/main' condition ensures that the site is only deployed when changes are pushed to the main branch.\nAfter the deploy.yml workflow is pushed, any future changes in the main branch, will trigger Hugo to generate an updated /public/ folder and serve it in GitHub Pages.\n","permalink":"https://tbalza.net/deploy-a-hugo-blog-on-github-pages-using-github-actions/","summary":"In this article we will go through the steps of setting up a static website, that will not incur in storage costs or egress fees.","title":"Deploy a Hugo Blog on Github Pages using Github Actions"}]