[{"content":"This guide provides a detailed walkthrough on how to create and deploy an Amazon API Gateway resource using Terraform. It also illustrates how to call a Lambda function via an HTTPS endpoint, which will interact directly with DynamoDB.\nOne of the key advantages of using Terraform is the ability to launch the entire project with a single click. The concept of infrastructure as code is vital for projects as it enables tracking of configuration modifications and promotes effective collaboration within large teams.\nFor Mac users, you can install Homebrew and set up AWS CLI, Terraform, and Bruno.\nbrew install awscli brew install terraform brew install bruno # An Opensource API client, we\u0026#39;ll use for testing aws configure # Follow the steps to provide your AWS account credentials (Do not run this in production, use a testing account instead)\ngit clone https://github.com/tbalza/lambda-dynamodb-api.git cd lambda-dynamodb-api terraform init This will initialize terraform, and download all the necessary libraries and modules, so that our HCL code in main.tf can interact with AWS using the credentials we set up earlier with the CLI. All the needed files will be stored in the .terraform folder for later use. terraform plan -out tfplan This command lays out all the changes that will take place, evaluating the current state of resources and what is indicated as an end result in our code. terraform apply -auto-approve tfplan After the apply is run, it will go ahead and create and configure our infrastructure as described in our code. after you\u0026rsquo;re done you can clean up by\nterraform destroy Ideally you want to terraform plan -destroy -out tfplan and then terraform apply tfplan as this allows you to double check all changes before commiting them to aws.\nBreak down of all the sections Using the commands provided above, you can swiftly set up and test the infrastructure. Additionally, you can easily clean it up to avoid any unexpected charges in the future. In the upcoming sections, we\u0026rsquo;ll provide a brief explanation of the function of each block.\nSetup IAM Permissions Create custom IAM policy module \u0026#34;iam_policy\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-policy\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; name = \u0026#34;lambda-apigateway-role-policy\u0026#34; description = \u0026#34;Custom policy with permission to DynamoDB and CloudWatch Logs\u0026#34; policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Sid = \u0026#34;Stmt1428341300017\u0026#34; Action = [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; }, { Sid = \u0026#34;\u0026#34; Resource = \u0026#34;*\u0026#34; Action = [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; } ] }) } This is a custom stand alone policy that allows the principal who assumes it to edit DynamoDB and CW Logs.\nCreate role for lambda function, attach custom IAM policy and trusted entity module \u0026#34;iam_assumable_role_lambda\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-assumable-role\u0026#34; version = \u0026#34;~\u0026gt; 5.37.1\u0026#34; create_role = true role_name = \u0026#34;lambda-apigateway-role\u0026#34; create_custom_role_trust_policy = true custom_role_trust_policy = data.aws_iam_policy_document.custom_trust_policy.json custom_role_policy_arns = [module.iam_policy.arn] # Get the ARN from the iam_policy module } # Create trusted entity data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;custom_trust_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } } } This creates a role, and attaches the previous policy to that role. Additionally it give the role the trusted entity required for the lambda role to assume it.\nCreate trigger equivalent to explicitly grant permissions to the API Gateway to invoke your Lambda function resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;apigw\u0026#34; { statement_id = \u0026#34;AllowExecutionFromAPIGateway\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.example.arn principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_api_gateway_deployment.dev.execution_arn}/*\u0026#34; } When creating resources via ClickOps, the trigger is automatically assigned to the lambda function when it is associated with the API, however via CLI we need to explicitly set this in order to have the correct permissions.\nCreate Lambda Function lambda_function.py from __future__ import print_function import boto3 import json print(\u0026#39;Loading function\u0026#39;) def lambda_handler(event, context): \u0026#39;\u0026#39;\u0026#39;Provide an event that contains the following keys: - operation: one of the operations in the operations dict below - tableName: required for operations that interact with DynamoDB - payload: a parameter to pass to the operation being performed \u0026#39;\u0026#39;\u0026#39; #print(\u0026#34;Received event: \u0026#34; + json.dumps(event, indent=2)) operation = event[\u0026#39;operation\u0026#39;] if \u0026#39;tableName\u0026#39; in event: dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;).Table(event[\u0026#39;tableName\u0026#39;]) operations = { \u0026#39;create\u0026#39;: lambda x: dynamo.put_item(**x), \u0026#39;read\u0026#39;: lambda x: dynamo.get_item(**x), \u0026#39;update\u0026#39;: lambda x: dynamo.update_item(**x), \u0026#39;delete\u0026#39;: lambda x: dynamo.delete_item(**x), \u0026#39;list\u0026#39;: lambda x: dynamo.scan(**x), \u0026#39;echo\u0026#39;: lambda x: x, \u0026#39;ping\u0026#39;: lambda x: \u0026#39;pong\u0026#39; } if operation in operations: return operations[operation](event.get(\u0026#39;payload\u0026#39;)) else: raise ValueError(\u0026#39;Unrecognized operation \u0026#34;{}\u0026#34;\u0026#39;.format(operation)) This lambda function is written in Python, and works with any table name we specify via the POST method. Actions such as, create, read, update, delete, list, echo and ping, are supported.\nZip the lambda_function.py to enable uploading to AWS data \u0026#34;archive_file\u0026#34; \u0026#34;lambda_zip\u0026#34; { type = \u0026#34;zip\u0026#34; source_file = \u0026#34;${path.module}/lambda_function.py\u0026#34; output_path = \u0026#34;${path.module}/lambda_function.zip\u0026#34; } We use built in TF tools to zip the Python script, which is needed in order to upload it to AWS via this method.\nCreate lambda function from lambda_function.py resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;example\u0026#34; { function_name = \u0026#34;LambdaFunctionsOverHttps\u0026#34; handler = \u0026#34;lambda_function.lambda_handler\u0026#34; runtime = \u0026#34;python3.12\u0026#34; filename = data.archive_file.lambda_zip.output_path # Associate function to previously created role role = module.iam_assumable_role_lambda.iam_role_arn # Get the ARN from the iam_assumable_role_lambda module } Finally, we upload the lambda function and attach the role we created earlier.\nDynamoDB Create a simple dynamodb table module \u0026#34;dynamodb_table\u0026#34; { source = \u0026#34;terraform-aws-modules/dynamodb-table/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0.1\u0026#34; name = \u0026#34;lambda-apigateway\u0026#34; hash_key = \u0026#34;id\u0026#34; # primary key attributes = [ { name = \u0026#34;id\u0026#34; type = \u0026#34;S\u0026#34; } ] } This creates a table with an \u0026ldquo;id\u0026rdquo; primary column that will expect strings.\nAPI Create API resource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;DynamoDBOperations\u0026#34; { name = \u0026#34;DynamoDBOperations\u0026#34; description = \u0026#34;API for DynamoDB Operations\u0026#34; api_key_source = \u0026#34;HEADER\u0026#34; endpoint_configuration { types = [\u0026#34;REGIONAL\u0026#34;] } } This creates de API by itself, and defines the end point configuration type.\nCreate resource resource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;DynamoDBManager\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id parent_id = aws_api_gateway_rest_api.DynamoDBOperations.root_resource_id path_part = \u0026#34;dynamodbmanager\u0026#34; } These are the various parts of your API that clients can access. They are represented as a hierarchical structure similar to a file path. For example, in the API endpoint https://api.example.com/users/profile, \u0026lsquo;users\u0026rsquo; and \u0026lsquo;profile\u0026rsquo; are resources. Resources can have child resources, creating a tree-like structure.\nCreate POST method resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;post\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = \u0026#34;POST\u0026#34; authorization = \u0026#34;NONE\u0026#34; api_key_required = false } These are the HTTP methods (also known as verbs) that clients can use to interact with the resources. Common methods include GET, POST, PUT, DELETE, and PATCH. Each method represents a different type of operation that can be performed on a resource. For example, a GET method on a \u0026lsquo;users\u0026rsquo; resource might retrieve a list of users, while a POST method on the same resource might create a new user.\nLink API to Lambda function resource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;lambda\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method integration_http_method = \u0026#34;POST\u0026#34; type = \u0026#34;AWS\u0026#34; uri = aws_lambda_function.example.invoke_arn passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; } uri = aws_lambda_function.example.invoke_arn: This line sets the URI of the integrated backend. In this case, it\u0026rsquo;s a Lambda function, and the ARN (Amazon Resource Name) used to invoke the function is retrieved from another resource named example of type aws_lambda_function.\nCreate response code resource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;response\u0026#34; { rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = \u0026#34;200\u0026#34; } http_method = aws_api_gateway_method.post.http_method: This line sets the HTTP method for the method response. The method is retrieved from another resource of type aws_api_gateway_method with the name post.\nstatus_code = \u0026ldquo;200\u0026rdquo;: This line sets the HTTP status code for the method response. In this case, it\u0026rsquo;s \u0026ldquo;200\u0026rdquo;, which typically represents a successful HTTP request.\nIntegrate response code resource \u0026#34;aws_api_gateway_integration_response\u0026#34; \u0026#34;lambda\u0026#34; { depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id resource_id = aws_api_gateway_resource.DynamoDBManager.id http_method = aws_api_gateway_method.post.http_method status_code = aws_api_gateway_method_response.response.status_code } depends_on = [aws_api_gateway_integration.lambda, aws_api_gateway_method_response.response]: This line specifies that the creation of this resource depends on the successful creation of other resources in order the execute correctly.\nThis creates an integration response that depends on the successful creation of the API Gateway integration and method response.\nBoth blocks are part of configuring an API Gateway to handle responses from a backend service, in this case, a Lambda function.\nDeploy in \u0026ldquo;Dev\u0026rdquo; resource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;dev\u0026#34; { depends_on = [aws_api_gateway_integration.lambda] rest_api_id = aws_api_gateway_rest_api.DynamoDBOperations.id stage_name = \u0026#34;dev\u0026#34; } This deploys our API to the \u0026ldquo;dev\u0026rdquo; stage, generating our invoke URL that we will use to interact with the API\nOutput API Invoke URL output \u0026#34;api_invoke_url\u0026#34; { description = \u0026#34;api_invoke_url\u0026#34; value = \u0026#34;${aws_api_gateway_deployment.dev.invoke_url}/${aws_api_gateway_resource.DynamoDBManager.path_part}\u0026#34; } This line from outputs.tf prints out our Invoke URL generated earlier, and displays it after terraform apply.\nTest with API Client Open Bruno. Create Collection \u0026gt; New Request \u0026gt; Past the invoke URL generated inside URL: POST\nThen in the BODY section, select Raw \u0026gt; JSON, past the code below, and press cmd+enter to execute\n{ \u0026#34;operation\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;lambda-apigateway\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1234ABCD\u0026#34;, \u0026#34;number\u0026#34;: 88 } } } Verify Results in DynamoDB Go to DynamoDB \u0026gt; Tables \u0026gt; lambda-apigateway \u0026gt; Explore Table Items Here you\u0026rsquo;ll see that our POST payload, sent to the API Invoke URL, interacts with Lambda, which in turn modifies our DynamoDB table.\nClean up Run this command to delete all our previously created resources and configurations.\nTake Aways We\u0026rsquo;ve created an API that is publicly accessible via our Invoke URL, which allows us to interact with the DynamoDB table directly via a Lambda function.\nIn future articles we\u0026rsquo;ll delve deeper into GitOps, and CI/CD pipelines building on this knowledge, and implement authentication, DNS and certificate configurations.\n","permalink":"https://tbalza.net/using-terraform-for-amazon-api-gateway-deployment/","summary":"This article offers a comprehensive tutorial on creating and deploying an Amazon API Gateway resource using Terraform, and demonstrates how to invoke a Lambda function through an HTTPS endpoint.","title":"Using Terraform for Amazon API Gateway Deployment"},{"content":"In this guide, we\u0026rsquo;ll walk you through the process of creating a static website that won\u0026rsquo;t burden you with storage costs or egress fees\nWe\u0026rsquo;ll also help you set up a custom domain that will automatically reflect any changes made whenever a new commit is detected in the main branch of our version control system.\nThe Problem with Egress Traffic Whether you\u0026rsquo;re operating an EC2 instance or directly serving files via S3, egress traffic can come with a price tag. For instance, AWS charges around $0.023 per GB. While this might not seem like much for personal pages, without the necessary precautions like DDoS protection or rate limiting, you could potentially face a hefty bill running into the thousands of dollars in the event of an attack. Fortunately, this won\u0026rsquo;t be an issue with our setup.\nThe Solution: Free Tier Services This is why it\u0026rsquo;s a smart move to utilize services like GitHub Pages and CloudFlare for a public personal site. These platforms don\u0026rsquo;t charge for outgoing traffic, even on their free tier service, making them an ideal solution.\nWhy Choose Hugo? Hugo is a static site generator written in Go. It\u0026rsquo;s not only incredibly fast but also simple to set up. The resulting pages load swiftly and are SEO optimized right from the start. This makes Hugo a fantastic alternative to platforms like WordPress, where publishing static content can be a bit of a hassle in terms of maintenance, costs, and performance.\nConfigure Your Personal Domain First, purchase your domain with services such as namecheap.com, then you can access the advanced settings and configure the records that will point to the GitHub Page that will reference to later Set up the A Records and CNAME just like in the image above. Reference the Github Pages Apex Configuration for further details. You can skip this step if you\u0026rsquo;d wish to deploy the site without a custom domain Set up Hugo Install Hugo and Create a New Site brew install hugo hugo new site tbalza-blog-hugo -f yml Install Hugo with Homebrew\nRun hugo new site \u0026lt;site_name\u0026gt;. This will create a directory \u0026lt;site_name\u0026gt; containing the hugo templates. Pass in the -f yml argument so configuration files are stored in the yml format\nInstall Theme git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) git submodule update --remote --merge Run git init on the root of the project to initialize a Git repository Install the PaperMod theme using Git Submodules theme: [\u0026#34;PaperMod\u0026#34;] Add the theme parameter in hugo.yaml baseURL: https://tbalza.net/ Add the baseURL parameter in hugo.yaml hugo server Run this command from within the project folder. This will create a blank page with no content Configure GitHub Actions to Publish to the GitHub Pages Push the First Commit, Create gh-pages Branch git commit -m \u0026#34;first commit\u0026#34; git branch -M main # rename master to main git remote add origin https://github.com/tbalza/tbalza-blog-hugo # replace with your repo git push -u origin main git branch gh-pages # used internally by github to execute the deployment action, will throw error if not created Create a repository on GitHub, add the remote address, and push your first commit, create gh-pages branch Allow Read and Write Permissions Under Settings \u0026gt; Actions \u0026gt; General \u0026gt; Workflow permissions, enable \u0026ldquo;Read and write permissions\u0026rdquo; Set up Custom Domain in GitHub Pages Configure your custom domain under Settings \u0026gt; Pages \u0026gt; Custom Domain Create deploy.yml mkdir -p .github/workflows cd .github/workflows touch deploy.yml In you project root folder create .github/workflows/deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.123.4/hugo_0.123.4_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;tbalza\u0026#39; git config user.email \u0026#39;tomas.balza@gmail.com\u0026#39; git commit -m \u0026#39;Updated site\u0026#39; git push Checkout source This action checks-out your repository under $GITHUB_WORKSPACE, so your workflow can access it. submodules:true ensures that our submodule for the theme repository is fetched as well Checkout destination The second step allows us to reference the gh-pages branch via the $GITHUB_WORKSPACE/built-site directory, where our static sites will be stored in Setup Hugo This step downloads and extracts the Hugo static site generator. It uses curl to download a specific version of Hugo from its GitHub releases page, and tar to extract the downloaded file. Build This step runs Hugo to build your static site. The built site\u0026rsquo;s files are placed in the public directory. Deploy This step deploys the built site to the gh-pages branch. It first copies the built site\u0026rsquo;s files from the public directory to the built-site directory. It then changes the current directory to built-site, stages all changes for commit with git add ., sets the Git username and email, commits the changes with a message of \u0026lsquo;Updated site\u0026rsquo;, and finally pushes the commit to the gh-pages branch. The gh-pages branch is special because GitHub Pages serves the contents of this branch at your GitHub Pages URL. The if: github.ref == 'refs/heads/main' condition ensures that the site is only deployed when changes are pushed to the main branch.\nAfter the deploy.yml workflow is pushed, any future changes in the main branch, will trigger Hugo to generate an updated /public/ folder and serve it in GitHub Pages.\n","permalink":"https://tbalza.net/deploy-a-hugo-powered-blog-on-github-pages-using-github-actions/","summary":"In this article we will go through the steps of setting up a static website, that will not incur in storage costs or egress fees.","title":"Deploy a Hugo-powered Blog on Github Pages using Github Actions"}]